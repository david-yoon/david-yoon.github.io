---
layout: page
title: "David Seunghyun Yoon"
---

<img src="{{ site.baseurl }}/assets/profile/img" align="right" title="Profile Picture" class="profile">
<!--<font size="5"><strong>David Seunghyun Yoon </strong></font> <br>-->
<br>
<font size="3">
	Research Scientist<br>
    <a href="https://research.adobe.com/">Adobe Research</a>, San Jose, CA, US<br><br>
    <!--
    <strong>NLP Research Scientist</strong><br>
	Adobe Research (San Jose, CA, US)
    -->
</font>



<br>
<br style="line-height:1.0em"> <a href="mailto:syoon@adobe.com">mysmilesh@gmail.com</a>
<br>[<a href="{{ site.baseurl }}/assets/cv.pdf">CV</a>]&nbsp;
    [<a href="https://scholar.google.co.kr/citations?hl=en&user=UpymOMwAAAAJ&view_op=list_works&sortby=pubdate">Google Scholar</a>]&nbsp;
    [<a href="http://github.com/david-yoon/">GitHub</a>]&nbsp;
    [<a href="http://www.linkedin.com/in/david-s-yoon/">LinkedIn</a>]&nbsp;
    [<a href="http://twitter.com/david_s_yoon/">twitter</a>]
<br>

<br><br>
<hr>


<p>
I am a Research Scientist at Adobe Research. My research interests are in the areas of <strong>machine learning</strong> and <strong>natural language processing (NLP)</strong>. I am particularly interested in understanding long texts for question answering systems and learning language representation for NLP tasks. Further interests lie in applying and integrating NLP research with other disciplines to tackle practical issues; understanding multimodal information (i.e., text, audio, and visual) and NLP for social good.
    
    
<p> I received my Ph.D. in Electrical and Computer Engineering from Seoul National University in 2020 with the Distinguished Dissertation Award, where I was fortunate to be advised by <a href="http://milab.snu.ac.kr/kjung/index.html" title="Kyomin Jung">Dr. Kyomin Jung</a>.    
Prior to Seoul National University, I had involved critical initiatives for the engineering and innovation of AI and machine learning while I was a staff software engineer at Samsung Research Artificial Intelligence Center (2006-2017).


<!--
<p>I have published studies at NLP, AI, or signal processing conferences, such as <strong>ACL</strong>, <strong>NAACL</strong>, <strong>AAAI</strong>, <strong>CIKM</strong>, and <strong>ICASSP</strong>.
</p>

<p>
    My research interests are in the areas of <strong>machine learning</strong> and <strong>natural language processing (NLP)</strong>. I am particularly interested in understanding long texts for question answering systems and learning language representation for NLP tasks. Further interests lie in applying and integrating NLP research with other disciplines to tackle practical issues; understanding multimodal information (i.e., text, audio, and visual) and NLP for social good.

<p>My  Ph.D. advisor is <a href="http://milab.snu.ac.kr/kjung/index.html" title="Kyomin Jung">Dr. Kyomin Jung</a>, Dept. of Electrical and Computer Engineering, Seoul National University. Before coming to SNU, I was a staff software engineer at Samsung Research AI Center.
    
I have published studies at NLP, AI, or signal processing conferences, such as <strong>ACL</strong>, <strong>NAACL</strong>, <strong>AAAI</strong>, <strong>CIKM</strong>, and <strong>ICASSP</strong>.
</p>
-->


<!--<p><strong>Keywords:</strong> NLP, Machine Learning, Artificial Intelligence</p>-->

<hr>


<h3>News</h3>

<ul style="line-height:1.4em">
  <font size="2">
 <li>
	<font color=red>*new*</font> [05/03]
	Two papers (HighGEN, MeetingQA) are accepted to <a href="https://2023.aclweb.org/">ACL 2023</a>.
  </li>
 <li>
	[01/20]
	One paper is accepted to <a href="https://2023.eacl.org/">EACL 2023</a>.
  </li>
 <li>
	[10/22]
	One paper is accepted to <a href="https://nips.cc/">NeurIPS 2022 Workshop on All Things Attention</a>.
  </li>
 <li>
	[10/22]
	One paper (GeNER) is accepted to <a href="https://2022.emnlp.org/">EMNLP 2022</a>.
  </li>
  <li>
	[09/22]
		I gave a talk at SKKU</a>,
		"Pretrained Language Model and Semantic Textual Understanding"
  </li>
 <li>
	[08/22]
	Five papers (KGQA, MedicalQA, Offensive content detection, Keyphrase extraction, Acronym extraction) are accepted to <a href="https://coling2022.org/">COLING 2022</a>.
  </li>
  <li>
	[08/22]
	Our patent (language model) has issued.
  </li>
  <li>
	[08/22]
		I gave a talk at <a href="http://capp.snu.ac.kr/?p=workshop#program">SNU GoGE Workshop 2022</a>,
		"Semantic Textual Understanding for Information Retrieval"
  </li>
  <li>
	[04/22]
	Two papers (image captioning, multimodal intent discovery) are accepted to <a href="https://2022.naacl.org/">NAACL 2022 Findings</a>.
  </li>
  <li>
	[04/22]
	Our paper (fake news detection) is accepted to <a href="https://lcs2.iiitd.edu.in/CONSTRAINT-2022/">ACL CONSTRAINT 2022</a>.
  </li>
  <li>
	[12/21]
	Our paper (Conversational Image Editing) is accepted to <a href="https://aaai.org/Conferences/AAAI-22/">AAAI 2022</a>.
  </li>
</ul>

<a href="{{ site.baseurl }}/news.html">history</a>
    
    
<hr>

<h3>Academic Activities</h3>
<ul style="line-height:1.4em">
  <li>
	<strong><font color=darkblue>Service: </font></strong> <br>
	<strong>Program Committee</strong>, NAACL (2019, 2021), ACL (2020, 2021), EMNLP (2019, 2020, 2021, 2022), AACL (2020, 2022), EACL (2021, 2023), COLING (2022) <br>
    <strong>Program Committee</strong>, AAAI (2020, 2021, 2022, 2023), WWW (2021), INTERSPEECH (2019) <br>
    <strong>Journal Reviewer</strong>, Information Processing and Management, 2020 <br>
    <strong>Journal Reviewer</strong>, IEEE Signal Processing Letters, 2020 <br>
    <br>
  </li>
  <li>
	<strong><font color=darkblue>Invited Talks: </font></strong> <br>
    Pretrained Language Model and Semantic Textual Understanding, <strong>SKKU</strong>, Sep. 2022<br>
    Semantic Textual Understanding for Information Retrieval, <a href="http://capp.snu.ac.kr/?p=workshop#program">Seoul National Univ.</a>, Aug. 2022<br>
    Mutimodal Evaluation Metric and Image Captioning Model, <strong>Korea Univ.</strong>, Dec. 2021<br>
    Recent Advancements in NLP for QA, LM, and Evaluation Metric, <strong>Dongguk Univ.</strong>, Sep. 2020<br>
    Understanding Long Texts for Question Answering System Using DNN, <strong>KAIST/IBS</strong>, Jul. 2020<br>
    Question Answering System for Long Text, <strong>Adobe Research</strong> (San Jose, CA, US), Dec. 2019<br>
    Question Answering System and Multimodal Speech Emotion Recognition, <strong>DEEPEST</strong>, Aug. 2019<br>
	Research in Natural Language Processing, 
	<a href="https://www.nvidia.com/ko-kr/ai-conference/"><strong>NVIDIA AI Conference</strong></a>, Jul. 2019<br>
	Question Answering for Short Answer, <strong>Adobe Research</strong> (San Jose, CA, US), Dec. 2018<br>
	QA-pair ranking algorithm and its applications, <strong>NAVER</strong>, Aug. 2018<br>
    Learning to Rank Question-Answer Pairs, <strong>PyTorch KR</strong>, Jun. 2018<br>
	Advancement of the Neural Dialogue Model, 
	<a href="https://www.fastcampus.co.kr/data_camp_lab/"><strong>Fast campus</strong></a>, 
	Jul. 2018 <br>
	<br>
  </li>
  <li><font size="2">
	<strong><font color=darkblue>Teaching Assistant: </font></strong> <br>
	Programming Methodology, Seoul National University, Spring 2018 <br>
	Machine Learning, Seoul National University, Fall 2015 <br>
	Lab. Sentiment Analysis, BigCamp (Big Data Academy), Big Data Institute, 2016-2019 <br>
	<br>
  </li>
  </font>
</ul>

<hr>

<h3>Professional Experiences</h3>
<ul style="line-height:1.4em">
	<li><font size="2">
	<strong><font color=darkblue>NLP Research Scientist: </font></strong>
	Adobe Research (San Jose, CA, US),
	2020-present
  </li>
  <li><font size="2">
	<strong><font color=darkblue>Research Scientist Intern: </font></strong>
	Adobe Research (San Jose, CA, US),
	Fall 2018 
  </li>
  <li>
	<strong><font color=darkblue>Staff Engineer: </font></strong>
	Samsung Research (Seoul, KR),
	2006-2017
  </li>
  <li>
	<strong><font color=darkblue>Representative of employees: </font></strong>
	Samsung Electronics (Seoul, KR),
	2012-2014
  </li>
  <li>
	<strong><font color=darkblue>Trainer of Global New Employee Course: </font></strong>
	Samsung Electronics (Seoul, KR),
	Spring 2011 
  </li></font></ul>
</ol>

<hr>

<!--<h3>Publications</h3>-->
<h3>Publications</h3>
<!--*denotes equal contribution.<br>-->
<ol style="line-height:1.4em" reversed>
  <font size="2">
  <h4><strong>[2023]</strong></h4>
	<li>    
        <strong>Automatic Creation of Named Entity Recognition Datasets by Querying Phrase Representations</strong>
        [<a href="https://arxiv.org/pdf/2210.07586.pdf">pdf</a>]
        <br><i>Hhynjae Kim, Jaehyo Yoo, <u>Seunghyun Yoon</u>, Jaewoo Kang</i>
        <br><a href="https://2023.aclweb.org/">ACL 2023</a>
        <p>
    </li>
	<li>    
        <strong>MEETINGQA: Extractive Question-Answering on Meeting Transcripts</strong>
        <br><i>Archiki Prasad, Trung Bui, <u>Seunghyun Yoon</u>, Hanieh Deilamsalehy, Franck Dernoncourt, Mohit Bansal
</i>
        <br><a href="https://2023.aclweb.org/">ACL 2023</a>
        <p>
    </li>
	<li>    
        <strong>PiC: A Phrase-in-Context Dataset for Phrase Understanding and Semantic Search</strong>
        [<a href="https://arxiv.org/pdf/2207.09068.pdf">pdf</a>]
        <br><i>Thang M. Pham, <u>Seunghyun Yoon</u>, Trung Bu, Anh Nguyeng</i>
        <br><a href="https://2023.eacl.org/">EACL 2023</a>
        <p>
    </li>
  <h4><strong>[2022]</strong></h4>
	<li>    
        <strong>Factual Error Correction for Abstractive Summaries Using Entity Retrieval</strong>
        [<a href="https://arxiv.org/pdf/2204.08263.pdf">pdf</a>]
        <br><i>Hwanhee Lee, Cheoneum Park, <u>Seunghyun Yoon</u>, Trung Bu, Franck Dernoncourt,  Juae Kim, Kyomin Jung</i>
        <br><a href="https://2022.emnlp.org/">EMNLP 2022 Workshop on GEM</a>
        <p>
    </li>
	<li>    
        <strong>Improving cross-modal attention via object detection</strong>
        [<a href="https://attention-learning-workshop.github.io/2022/papers/kim-improving_crossmodal_attention_via_object_detection.pdf">pdf</a>]
        <br><i>Yongil Kim, Yerin Hwang, <u>Seunghyun Yoon</u>, Hyeongu Yun, Kyomin Jung</i>
        <br><a href="https://nips.cc/">NeurIPS 2022 Workshop on All Things Attention</a>
        <p>
    </li>
	<li>    
        <strong>Simple Questions Generate Named Entity Recognition Datasets</strong>
        [<a href="https://arxiv.org/pdf/2112.08808.pdf">pdf</a>]
        [<a href="https://github.com/dmis-lab/GeNER">code</a>]      
        <br><i>Hyunjae Kim, Jaehyo Yoo, <u>Seunghyun Yoon</u>, Jinhyuk Lee, Jaewoo Kang</i>
        <br><a href="https://2022.emnlp.org/">EMNLP 2022</a>
        <p>
    </li>
	<li>    
        <strong>Virtual Knowledge Graph Construction for Zero-Shot Domain-Specific Document Retrieval</strong>
        [<a href="https://aclanthology.org/2022.coling-1.101.pdf">pdf</a>]
        <br><i>Yeon Seonwoo, <u>Seunghyun Yoon</u>, Franck Dernoncourt, Trung Bui, Alice Oh</i>
        <br><a href="https://coling2022.org/">COLING 2022</a>
        <p>
    </li>
	<li> 
        <strong>Medical Question Understanding and Answering with Knowledge Grounding and Semantic Self-Supervision</strong>
        [<a href="https://aclanthology.org/2022.coling-1.241.pdf">pdf</a>]
        <br><i>Khalil Mrini, Harpreet Singh, Franck Dernoncourt, <u>Seunghyun Yoon</u>, Trung Bui, Walter W. Chang, Emilia Farcas, Ndapa Nakashole</i>
        <br><a href="https://coling2022.org/">COLING 2022</a>
        <p>
    </li>
	<li> 
        <strong>Offensive Content Detection Via Synthetic Code-Switched Text</strong>
        [<a href="https://aclanthology.org/2022.coling-1.575.pdf">pdf</a>]
        <br><i>Cesa Salaam, Franck Dernoncourt, Trung Bui, <u>Seunghyun Yoon</u></i>
        <br><a href="https://coling2022.org/">COLING 2022</a>
        <p>
    </li>
	<li> 
        <strong>Keyphrase Prediction from Video Transcripts: New Dataset and Directions</strong>
        [<a href="https://aclanthology.org/2022.coling-1.624.pdf">pdf</a>]
        <br><i>Amir Pouran Ben Veyseh, Quan Hung Tran, <u>Seunghyun Yoon</u>, Varun Manjunatha, Hanieh Deilamsalehy, Rajiv Jain, Trung Bui, Walter W. Chang, Franck Dernoncourt, Thien Huu Nguyen</i>
        <br><a href="https://coling2022.org/">COLING 2022</a>
        <p>
    </li>
	<li> 
        <strong>MACRONYM: A Large-Scale Dataset for Multilingual and Multi-Domain Acronym Extraction</strong>
        [<a href="https://aclanthology.org/2022.coling-1.292.pdf">pdf</a>]
        <br><i>Amir Pouran Ben Veyseh, Nicole Meister, <u>Seunghyun Yoon</u>, Rajiv Jain, Franck Dernoncourt, Thien Huu Nguyen
</i>
        <br><a href="https://coling2022.org/">COLING 2022</a>
        <p>
    </li>
    <li>    
        <strong>Fine-grained Image Captioning with CLIP Reward</strong>
        [<a href="https://aclanthology.org/2022.findings-naacl.39.pdf">pdf</a>]
        [<a href="https://github.com/j-min/CLIP-Caption-Reward">code</a>]
        [<a href="https://huggingface.co/spaces/NAACL2022/CLIP-Caption-Reward">demo</a>]
        <br><i>Jaemin Cho, <u>Seunghyun Yoon</u>, Ajinkya Kale, Franck Dernoncourt, Trung Bui, M Bansal</i>
        <br><a href="https://2022.naacl.org/">NAACL Findings 2022</a>
        <p>
    </li>
    <li>    
        <strong>Multimodal Intent Discovery from Livestream Videos</strong>
        [<a href="https://aclanthology.org/2022.findings-naacl.36.pdf">pdf</a>]
        [<a href="https://github.com/adymaharana/VideoIntentDiscovery">code</a>]
        <br><i>Adyasha Maharana, Quan Tran, <u>Seunghyun Yoon</u>, Franck Dernoncourt, Trung Bui, Walter Chang, M Bansal</i>
        <br><a href="https://2022.naacl.org/">NAACL Findings 2022</a>
        <p>
    </li>
    <li>
        <strong>How does fake news use a thumbnail? CLIP-based Multimodal Detection on the Unrepresentative News Image</strong>
        [<a href="https://aclanthology.org/2022.constraint-1.10.pdf">pdf</a>]
        <br><i>Hyewon Choi, Yejun Yoon, <u>Seunghyun Yoon</u>, Kunwoo Park</i>
        <br><a href="https://lcs2.iiitd.edu.in/CONSTRAINT-2022/">ACL CONSTRAINT 2022</a>
        <p>
    </li>
    <li>
        <strong>CAISE: Conversational Agent for Image Search and Editing</strong>
			  [<a href="https://arxiv.org/abs/2202.11847">pdf</a>]
        [<a href="https://github.com/hyounghk/CAISE">code</a>]
			<br><i>Hyounghun Kim, Doo Soon Kim, <u>Seunghyun Yoon</u>, Franck Dernoncourt, Trung Bui, Mohit Bansal</i>
        <br><a href="https://aaai.org/Conferences/AAAI-22/">AAAI 2022</a>
        <p>
    </li>
  <h4><strong>[2021]</strong></h4>
    <li >    
        <strong>Few-Shot Intent Detection via Contrastive Pre-Training and Fine-Tuning</strong>
		[<a href="https://arxiv.org/abs/2109.06349">pdf</a>]
        <br><i>J Zhang, T Bui, <u>S Yoon</u>, X Chen, Z Liu, C Xia, QH Tran, W Chang, P Yue</i>
        <br><a href="https://2021.emnlp.org/">EMNLP 2021</a>
        <p>
    </li>
    <li >    
        <strong>QACE: Asking Questions to Evaluate an Image Caption</strong>
		[<a href="https://arxiv.org/abs/2108.12560">pdf</a>]
        [<a href="https://github.com/hwanheelee1993/QACE">code</a>]
        <br><i>H Lee, T Scialom, <u>S Yoon</u>, F Dernoncourt, K Jung</i>
        <br><a href="https://2021.emnlp.org/">EMNLP Findings 2021</a>
        <p>
    </li>  
    <li >    
        <strong>A Gradually Soft Multi-Task and Data-Augmented Approach to Medical Question Understanding</strong>
		[<a href="https://aclanthology.org/2021.acl-long.119.pdf">pdf</a>]
        <br><i>K Mrini, F Dernoncourt, <u>S Yoon</u>, T Bui, W Chang, E Farcas, N Nakashole</i>
        <br><a href="https://2021.aclweb.org/">ACL 2021</a>
        <p>
    </li>
    <li >
        <strong>UMIC: An Unreferenced Metric for Image Captioning via Contrastive Learning</strong>
		[<a href="https://aclanthology.org/2021.acl-short.29.pdf">pdf</a>]
        [<a href="https://github.com/hwanheelee1993/UMIC">code</a>]
        <br><i>H Lee, <u>S Yoon</u>, F Dernoncourt, T Bui, K Jung</i>
        <br><a href="https://2021.aclweb.org/">ACL 2021</a>
        <p>
    </li>
    <li >
        <strong>UCSD-Adobe at MEDIQA 2021: Transfer Learning and Answer Sentence Selection for Medical Summarization</strong>
        [<a href="https://www.aclweb.org/anthology/2021.bionlp-1.28.pdf">pdf</a>]
        <br><i>K Mrini, F Dernoncourt, <u>S Yoon</u>, T Bui, W Chang, E Farcas, N Nakashole</i>
        <br><a href="https://aclweb.org/aclwiki/BioNLP_Workshop">NAACL BioNLP 2021</a>
        <p>
    </li>
    <li >
	<strong>KPQA: A Metric for Generative Question Answering Using Keyphrase Weights</strong>
    [<a href="https://www.aclweb.org/anthology/2021.naacl-main.170.pdf">pdf</a>]
    [<a href="https://github.com/hwanheelee1993/KPQA">code / checkpoint</a>]            
    <br><i>H Lee, <u>S Yoon</u>, F Dernoncourt, DS Kim, T Bui, J Shin, K Jung</i>
	<br><a href="https://2021.naacl.org/">NAACL 2021</a>
    <p>
    </li>
    <li >
	<strong>Learning to Detect Incongruence in News Headline and Body Text via a Graph Neural Network</strong>
    [<a href="https://ieeexplore.ieee.org/document/9363185">pdf</a>]
    [<a href="https://github.com/minwhoo/detecting-incongruity-gnn">code</a>]            
   	<br>(<font color=orange>SCIE, IF=3.745</font>)
    <br><i><u>S Yoon</u><sup>*</sup>, K Park<sup>*</sup>, M Lee, T Kim, M Cha, K Jung</i>
	<br><a href="https://ieeeaccess.ieee.org/">IEEE Access 2021</a>
    <p>
  </li>
  <h4><strong>[2020]</strong></h4>
    <li >
	<strong>Collaborative Training of GANs in Continuous and Discrete Spaces for Text Generation</strong>
    [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9296209">pdf</a>]  
   	<br>(<font color=orange>SCIE, IF=3.745</font>)
    <br><i>Y Kim, S Won, <u>S Yoon</u>, K Jung</i>
	<br><a href="https://ieeeaccess.ieee.org/">IEEE Access 2020</a>
    <p>
  </li>
  <li >
	<strong>ViLBERTScore: Evaluating Image Caption Using Vision-and-Language BERT</strong>
    [<a href="https://www.aclweb.org/anthology/2020.eval4nlp-1.4.pdf">pdf</a>]
    [<a href="https://github.com/hwanheelee1993/ViLBERTScore">code / checkpoint</a>]    
	<br><i>H Lee, <u>S Yoon</u>, F Dernoncourt, DS Kim, T Bui, K Jung</i>
  <br><a href="https://nlpevaluation2020.github.io/index.html">EMNLP Eval4NLP 2020</a>
  <p>
  </li>
  <li >
	<strong>Multimodal Speech Emotion Recognition using Cross Attention with Aligned Audio and Text</strong>
    [<a href="https://arxiv.org/pdf/2207.12895.pdf">pdf</a>]
  	<br><i>Y Lee, <u>S Yoon</u>, K Jung</i>
	<br><a href="https://arxiv.org/abs/2207.12895/">INTERSPEECH 2020</a>
    <p>
  </li>
  <li >
	<strong>Fast and Accurate Deep Bidirectional Language Representations for Unsupervised Learning</strong>
      [<a href="https://www.aclweb.org/anthology/2020.acl-main.76.pdf">pdf</a>]
      [<a href="https://github.com/joongbo/tta">code / checkpoint</a>]
    <br>(<font color=orange>acceptance rate: 25.2%</font>)
  	<br><i>J Shin, Y Lee, <u>S Yoon</u>, K Jung</i>
	<br><a href="https://acl2020.org/">ACL 2020</a>
    <p>
  </li> 
  <li >
	<strong>Propagate-Selector: Detecting Supporting Sentences for Question Answering via Graph Neural Networks</strong>
	[<a href="https://www.aclweb.org/anthology/2020.lrec-1.664">pdf</a>]
    [<a href="https://github.com/david-yoon/propagate-selector">code</a>]
  	<br><i><u>S Yoon</u>, F Dernoncourt, DS Kim, T Bui, K Jung</i>
	<br><a href="https://lrec2020.lrec-conf.org/">LREC 2020</a>
    <p>
  </li>
  <li >
	<strong>Drug-disease Graph: Predicting Adverse Drug Reaction Signals via Graph Neural Network with Clinical Data</strong>
      [<a href="https://arxiv.org/pdf/2004.00407.pdf">pdf</a>]
      [<a href="https://pakdd2020.org/download/conference_paper_slides/main-851.pdf">slide</a>]
  	<br>(<font color=orange>oral presentation, accpetance rate: 21%</font>)
    <br><i>H Kwak, M Lee, <u>S Yoon</u>, J Chang, S Park, K Jung</i>
	<br><a href="https://pakdd2020.org/">PAKDD 2020</a>
    <p>
  </li>
  <li >
	<strong>DSTC8-AVSD: Multimodal Semantic Transformer Network with Retrieval Style Word Generator</strong>
	[<a href="https://arxiv.org/pdf/2004.08299.pdf">pdf</a>]
	<br><i>H Lee, <u>S Yoon</u>, F Dernoncourt, DS Kim, T Bui, K Jung</i>
  <br><a href="https://aaai.org/Conferences/AAAI-20/ws20workshops/#ws09">AAAI 2020 DSTC8</a>
  <p>
  </li>
  <li >
	<strong>Comparative Studies on Machine Learning for Paralinguistic Signal Compression and Classification</strong>
    [<a href="https://link.springer.com/content/pdf/10.1007/s11227-020-03346-3.pdf">pdf</a>]        
   	<br>(<font color=orange>SCI, IF=2.157</font>)
    <br><i>S Byun<sup>*</sup>, <u>S Yoon</u><sup>*</sup>, K Jung</i>
	<br><a href="https://www.springer.com/journal/11227/?gclid=EAIaIQobChMIrJqOzti96QIVBhdgCh1UVAn_EAAYASAAEgLwp_D_BwE/">Journal of Supercomputing 2020</a>
    <p>
  </li> 
  <li >
	<strong>Attentive Modality Hopping Mechanism for Speech Emotion Recognition</strong>
      [<a href="{{ site.baseurl }}/assets/paper/yoon2020attentive.pdf">pdf</a>]
      [<a href="https://github.com/david-yoon/attentive-modality-hopping-for-SER">code</a>]
      [<a href="https://www.slideshare.net/DavidSeunghyunYoon/slide-attentive-modality-hopping-mechanism-for-speech-emotion-recognition">slide</a>]
	<br>(<font color=orange>oral presentation</font>)
    <br><i><u>S Yoon</u>, S Dey, H Lee, K Jung</i>
	<br><a href="https://2020.ieeeicassp.org/">IEEE ICASSP 2020</a>
    <p>
  </li>
  <li >
	<strong>BaitWatcher: A lightweight web interface for the detection of incongruent news headlines</strong>
    [<a href="https://arxiv.org/pdf/2003.11459.pdf">pdf</a>]
    [<a href="https://www.springer.com/gp/book/9783030426989">book</a>]
  	<br><i>K Park, T Kim, <u>S Yoon</u>, M Cha, K Jung</i>
	<br>Disinformation, Misinformation, and Fake News in Social Media-Emerging Research Challenges and Opportunities, <a href="https://www.springer.com/gp/book/9783030426989">Springer 2020</a>
    <p>
  </li>
      
  <strong><h4>[2019]</h4></strong>
  <li >
	<strong>A Compare-Aggregate Model with Latent Clustering for Answer Selection</strong>
	[<a href="https://arxiv.org/pdf/1905.12897.pdf">pdf</a>]
  [<a href="https://www.slideshare.net/DavidSeunghyunYoon/slide-a-compareaggregate-model-with-latent-clustering-for-answer-selection">slide</a>]
  [<a href="https://www.slideshare.net/DavidSeunghyunYoon/poster-a-compareaggregate-model-with-latent-clustering-for-answer-selection">poster</a>]
  <br>(<font color=orange>oral presentation, accpetance rate: 21.2%</font>)
	<br><i><u>S Yoon</u>, F Dernoncourt, DS Kim, T Bui, K Jung</i>
	<br><a href="http://www.cikm2019.net/">CIKM 2019</a>
    <p>
  </li>
  <li >
	<strong>Surf at MEDIQA 2019: Improving Performance of Natural Language Inference in the Clinical Domain by Adopting Pre-trained Language Model</strong>
  [<a href="https://www.aclweb.org/anthology/W19-5043.pdf">pdf</a>]
  [<a href="https://www.slideshare.net/jiinnam1/2019-acl-bionlpnlisurfposter">poster</a>]	
	<br><i>J Nam, <u>S Yoon</u>, K Jung</i>
	<br><a href="https://aclweb.org/aclwiki/BioNLP_Workshop">ACL BioNLP 2019</a>
    <p>
  </li>
  <li >
	<strong>Speech Emotion Recognition Using Multi-hop Attention Mechanism</strong>
	[<a href="https://arxiv.org/pdf/1904.10788.pdf">pdf</a>]
    [<a href="https://sigport.org/documents/speech-emotion-recognition-using-multi-hop-attention-mechanism">slide</a>]
	<br>(<font color=orange>oral presentation</font>)
	<br><i><u>S Yoon</u>, S Byun, S Dey, K Jung</i>
	<br><a href="https://2019.ieeeicassp.org/">IEEE ICASSP 2019</a>
    <p>
  </li>
  <li >
	<strong>Neural Networks for Compressing and Classifying Speaker-Independent Paralinguistic Signals</strong>
	[<a href="http://milab.snu.ac.kr/pub/BigComp2019Byun.pdf">pdf</a>]
	<br><i>S Byun, <u>S Yoon</u>, K Jung</i>
	<br><a href="http://www.bigcomputing.org/">IEEE BigComp 2019</a>
    <p>
  </li>
	<li >
	<strong>Detecting Incongruity Between News Headline and Body Text via a Deep Hierarchical Encoder</strong>
	[<a href="https://wvvw.aaai.org/ojs/index.php/AAAI/article/view/3756/3634">pdf</a>]
	[<a href="https://github.com/david-yoon/detecting-incongruity">code</a>]
    [<a href="https://www.slideshare.net/DavidSeunghyunYoon/detecting-incongruity-between-news-headline-and-body-text-via-a-deep-hierarchical-encoder">slide</a>]
    [<a href="https://www.slideshare.net/DavidSeunghyunYoon/poster-detecting-incongruity-between-news-headline-and-body-text-via-a-deep-hierarchical-encoder">poster</a>]
  <br>(<font color=orange>oral presentation, accpetance rate: 16.2%</font>)
	<br><i><u>S Yoon</u><sup>*</sup>, K Park<sup>*</sup>, J Shin, H Lim, S Won, M Cha, K Jung</i>
	<br><a href="https://aaai.org/Conferences/AAAI-19/">AAAI 2019</a>
    <p>
  </li>
      
  <strong><h4>[2018 and earlier]</h4></strong>
  <li >
	<strong>Multimodal Speech Emotion Recognition using Audio and Text</strong>
	[<a href="https://arxiv.org/pdf/1810.04635.pdf">pdf</a>]
    [<a href="https://github.com/david-yoon/multimodal-speech-emotion">code</a>]
    [<a href="https://www.slideshare.net/DavidSeunghyunYoon/multimodal-speech-emotion-recognition-using-audio-and-text">poster</a>]
	<br><i><u>S Yoon</u>, S Byun, K Jung</i>
	<br><a href="http://www.slt2018.org/">IEEE SLT 2018</a>
    <p>
  </li>
  <li >
	<strong>Comparative Studies of Detecting Abusive Language on Twitter</strong>	
	[<a href="https://www.aclweb.org/anthology/W18-5113.pdf">pdf</a>]
 	[<a href="https://github.com/younggns/comparative-abusive-lang">code</a>]
	&nbsp
	<br><i>Y Lee<sup>*</sup>, <u>S Yoon</u><sup>*</sup>, K Jung</i>
	<br><a href="https://sites.google.com/view/alw2018">EMNLP ALW 2018</a>
    <p>
  </li>
  <li >
	<strong>Learning to Rank Question-Answer Pairs using Hierarchical Recurrent Encoder with Latent Topic Clustering</strong>
	[<a href="https://www.aclweb.org/anthology/N18-1142.pdf">pdf</a>] 
	[<a href="https://github.com/david-yoon/QA_HRDE_LTC">code</a>]
	[<a href="https://www.slideshare.net/DavidSeunghyunYoon/learning-to-rank-questionanswer-pairs-using-hierarchical-recurrent-encoder-with-latent-topic-clustering">poster</a>]
  	[<a href="https://youtu.be/k3rAEM91wfM?list=PLpnJjnJBNxUMFwlC0Wjm51FJZDXbyP4ad">video_kor</a>]
  	<br>(<font color=orange>acceptance rate: 31%</font>)
	<br><i><u>S Yoon</u>, J Shin, K Jung</i>
	<br><a href="http://naacl2018.org/">NAACL 2018</a>
    <p>
  </li>
  <li>
	<strong>Contextual-CNN: A Novel Architecture Capturing Unified Meaning for Sentence Classification</strong>	
	[<a href="http://milab.snu.ac.kr/pub/BigComp2018.pdf">pdf</a>]	
	<br><i>J Shin, Y Kim, <u>S Yoon</u>, K Jung</i>
	<br><a href="http://www.bigcomputing.org/">IEEE BigComp 2018</a>
    <p>
  </li>
  <li>
	<strong>Synonym Discovery with Etymology-based Word Embeddings</strong></a>	
	[<a href="https://arxiv.org/pdf/1709.10445.pdf">pdf</a>]	
	<br><i><u>S Yoon</u>, P Estrada, K Jung</i>
	<br><a href="http://www.ele.uri.edu/ieee-ssci2017/">IEEE SSCI 2017</a>
    <p>
  </li>
  <li>
	<strong>Efficient Transfer Learning Schemes for Personalized Language Modeling using Recurrent Neural Network</strong>	
	[<a href="http://milab.snu.ac.kr/pub/AAAI2017Yoon.pdf">pdf</a>]	
	<br><i><u>S Yoon</u>, H Yun, Y Kim, G Park, K Jung</i>
	<br><a href="http://crowdai.azurewebsites.net/">AAAI 2017 (Workshop)</a>
    <p>
  </li>
  <li>
	<strong>Automatic Question Answering System for Consumer Product</strong>	
	[<a href="http://milab.snu.ac.kr/pub/IntelliSys2016Yoon.pdf">pdf</a>]	
	<br><i><u>S Yoon</u>, M Sundar, A Gupta, K Jung</i>
	<br><a href=http://saiconference.com/Conferences/IntelliSys2016">IntelliSys 2016</a>
    <p>
  </li>
  <li>
	<strong>Mining the Minds of Customers from Online Chat Logs</strong>
	[<a href="https://arxiv.org/abs/1510.01801">pdf</a>]
	<br>(<font color=orange>accpetance rate: 21%</font>)
	<br><i>K Park, J Kim, J Park, M Cha, J Nam, <u>S Yoon</u>, E Rhim</i>
	<br><a href="http://www.cikm-2015.org/">CIKM 2015</a>
    <p>
  </li>
  <li>
	<strong>Domain Question Answering System</strong>
	<br><i><u>S Yoon</u>, E Rhim, D Kim</i>
	<br>KIISE Transactions on Computing Practices 2015
    <p>
  </li>
  <li>
	<strong>Media clips: Implementation of an intuitive media linker</strong>	
	[<a href="{{ site.baseurl }}/assets/paper/MediaClip_BMSB2011.pdf">pdf</a>]	
	<br><i><u>S Yoon</u>, K Lee, H Shin</i>
	<br><a href="https://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp?punumber=5945017">IEEE BMSB 2011</a>
    <p>
  </li>
  </font>
  </ol>  

        
        
<hr>

<h3>Patents</h3>
<font color=lightblue>
<h4>[ International Patents ]</h4>
</font>
<ol reversed style="line-height:1.4em">
  <font size="2">
    <li>
		[<font color=magenta>issued</font>]
	 <strong>Utilizing a graph neural network to identify supporting text phrases and generate digital query responses</strong>
	[<a href="https://patents.google.com/patent/US20210058345A1/en?oq=US20210058345A1">link</a>]	
        <br><i><u>S Yoon</u>, F Dernoncourt, DS Kim, T Bui</i>
	<br>US 11,271,876, Mar. 8, 2022
    <p>
    </li>
    <li>
    [<font color=magenta>issued</font>]
	<strong>Utilizing bi-directional recurrent encoders with multi-hop attention for speech emotion recognition</strong>
	[<a href="https://patents.google.com/patent/US20210050033A1/en?oq=US20210050033A1">link</a>]	
        <br><i>T Bui, S Dey, <u>S Yoon</u></i>
	<br>US 11,205,444, Dec. 21, 2021
    <p>
    </li>
    <li>
    [<font color=magenta>issued</font>]
	<strong>Answer selection using a compare-aggregate model with language model and condensed similarity information from latent clustering</strong>
	[<a href="https://patents.google.com/patent/US11113323B2/en?oq=US11113323B2">link</a>]	
        <br><i><u>S Yoon</u>, F Dernoncourt, T Bui, DS Kim, CI Dockhorn, Y Gong</i>
	<br>US 11,113,323, Sep. 7, 2021
    <p>
    </li>
    <li>
    [<font color=magenta>issued</font>]	
	<strong>Terminal apparatus, server and method of controlling the same</strong>
	[<a href="https://patents.google.com/patent/US10084850B2/en?oq=US+10%2c084%2c850/">link</a>]	
	<br><i>Y Kim, O Kwon, S Kim, H Oh, <u>S Yoon</u>, S Cha, J Lee</i>
	<br>US 10,084,850, CN 201410085759, EP20140154718, Sep. 25, 2018
    <p>
  </li> 
  <li>
	<strong>Method and device for analyzing user's emotion</strong>
    [<a href="https://patentscope.wipo.int/search/en/detail.jsf?docId=WO2016182393">link</a>]	
	<br><i>E Rhim, J Kim, J Nam, <u>S Yoon</u>, K Park, J Park, M Cha</i>
	<br>WO2016182393, May. 13, 2016
    <p>
  </li> 
  <li>
	[<font color=magenta>issued</font>]	
	<strong>Method of recommending application, mobile terminal using the method, and communication system using the method</strong>
	[<a href="https://patents.google.com/patent/US9247376B2/en?oq=US+9%2c247%2c376">link</a>]	
	<br><i>J Nam, M Lee, M Koo, <u>S Yoon</u></i>
	<br>US 9,247,376, Jan. 26, 2016 &nbsp
    <p>
  </li> 
  <li>
	[<font color=magenta>issued</font>]&nbsp
	<strong>Method and apparatus for displaying photo on screen having any shape</strong>
	[<a href="https://patents.google.com/patent/US9049383B2/en?oq=US+9%2c049%2c383">link</a>]	
	<br><i><u>S Yoon</u>, M Lee</i>
	<br>US 9,049,383, Jun. 2, 2015 &nbsp
	<p>
  </li> 
  <li>
	[<font color=magenta>issued</font>]&nbsp
	<strong>Method and apparatus for providing information and computer readable storage medium having a program recorded thereon for executing the method</strong>
	[<a href="https://patents.google.com/patent/US8958824B2/en?oq=US+8%2c958%2c824">link</a>]	
	<br><i><u>S Yoon</u>, M Lee, M Koo, J Nam</i>
	<br>US 8,958,824, Feb. 17, 2015 &nbsp
    <p>
  </li> 
  <li>
    [<font color=magenta>issued</font>]&nbsp
	<strong>Apparatus and method for clipping and sharing content at a portable terminal</strong>
	[<a href="https://patents.google.com/patent/CN103827913A/en?oq=CN103827913A">link</a>]
	<br><i><u>S Yoon</u>, M Lee, M Koo, J Nam</i>
	<br>US 13/629,394, CN103827913A, EP20120837007, PCT/KR1020110097578, May. 28, 2014
    <p>
  </li>
  <li>
	[<font color=magenta>issued</font>]&nbsp
	<strong>Method and apparatus for fast tracking position by using global positioning system</strong>
	[<a href="https://patents.google.com/patent/US8094070B2/en?oq=US8094070B2">link</a>]	
	<br><i><u>S Yoon</u>, S Kim</i>
	<br>US 8,094,070, Jan. 10, 2012 &nbsp
    <p>
  </li>
<!-- 
  <li>
	<strong>User terminal device, information providing system, and method for providing information</strong>
	[<a href="https://patents.google.com/patent/US20150106297A1/">link</a>]		
	<br><i>Y Won, <u>S Yoon</u>, O Kwon, M Kim, H Oh, S Lee, S Cha</i>
	<br>US 14/511,506, EP20140185800, Oct. 10, 2014
    <p>
  </li>
  <li>
	<strong>Apparatus and method for searching with consideration user's action</strong>
	[<a href="https://patents.google.com/patent/US20130086055?oq=13%2f631%2c353">link</a>]
	<br><i><u>S Yoon</u>, M Lee, M Koo, J Nam</i>
	<br>US 13/631,353, Sep. 28, 2012
    <p>
  </li>
-->
<!-- 
  <li>
	<strong>Apparatus and method for correcting position information of portable terminal in multi-path zone</strong>
	[<a href="https://patents.google.com/patent/US20130057430?oq=13%2f604528">link</a>]
	<br><i><u>S Yoon</u></i>
	<br>US 13/604,528, Sep. 5, 2012
    <p>
  </li>
  <li>
	<strong>Method and apparatus for connecting devices</strong>
	[<a href="https://patents.google.com/patent/US20120271901?oq=13%2f452%2c766">link</a>]
	<br><i>K Kim, J Lee, W Park, H Shim, Y Park, M Lee, M Koo, <u>S Yoon</u>, J Nam</i>
	<br>US 13/452,766, EP20120162473, PCT/KR1020110037351 , Apr. 20, 2012
    <p>
  </li>
  <li>
	<strong>Method and apparatus for crawling webpages</strong>
	[<a href="https://patents.google.com/patent/US20120102019?oq=13%2f116%2c785">link</a>]
	<br><i><u>S Yoon</u>, S Maeng, J Huh, S Seo, J Kim, J Park</i>
	<br>US 13/116,785, May. 26, 2011
    <p>
  </li>
-->
</font>
</ol>


<font color=lightblue>
<h4>[ Korean Patents ]</h4>
</font>
<ol reversed style="line-height:1.4em">
  <font size="2">
  <li>
	[<font color=magenta>issued</font>]&nbsp	
	<strong>Apparatus and method for evaluating sentense by using bidirectional language model</strong>
	<br><i>K Jung, J Shin, <u>S Yoon</u></i>
	<br>KR 10-2436900, Aug. 23, 2022
    <p>
  </li> 
	<li>
	[<font color=magenta>issued</font>]&nbsp
	<strong>Method and apparatus for emotion recognition based on cross attentionmodel</strong>
	<br><i>K Jung, Y Lee, <u>S Yoon</u></i>
	<br>KR 10-2365433, Feb. 16, 2022
    <p>
  </li>
  <li>
  [<font color=magenta>issued</font>]&nbsp
	<strong>Artificial intelligence based dialog system and response control method thereof</strong>
	<br><i>K Jung, <u>S Yoon</u>, J Shin, H Kwak, S Byun</i>
	<br>KR 10-2059015, Dec. 18, 2019
    <p>
  </li>
  <li>
	[<font color=magenta>issued</font>]&nbsp
	<strong>Terminal apparatus, server and method of controlling the same</strong>
	<br><i>Y Kim, O Kwon, S Kim, H Oh, <u>S Yoon</u>, S Cha, J Lee</i>
	<br>KR 10-1832394, Feb. 20, 2018
    <p>
  </li> 
  <li>
	[<font color=magenta>issued</font>]&nbsp
	<strong>Apparatus and method for collecting information of destination in portable terminal</strong>
	<br><i><u>S Yoon</u>, J Nam, M Koo, M Lee</i>
	<br>KR 10-1914632, Oct. 29, 2018
    <p>
  </li> 
  <li>
	[<font color=magenta>issued</font>]&nbsp
	<strong>Method and apparatus for providing information, and computer readable storage medium</strong>
	<br><i><u>S Yoon</u>, M Lee, M Koo, J Nam</i>
	<br>KR 10-1773167, Aug. 24, 2017 &nbsp
    <p>
  </li> 
  <li>
	[<font color=magenta>issued</font>]
	<strong>Method for recommendation of application, mobile terminal thereof and communication system thereof</strong>
	<br><i>J Nam, M Lee, M Koo, <u>S Yoon</u></i>
	<br>KR 10-1747303, Jun. 8, 2017 &nbsp
    <p>
  </li> 
  <li>
	<strong>Device and method for analyzing user emotion</strong>
        <br><i>E Rhim, J Kim, J Nam, <u>S Yoon</u>, K Park, J Park, M Cha</u></i>
	<br>KR 1020160058782, May. 13, 2016 &nbsp
    <p>
  </li> 
  <li>
	[<font color=magenta>issued</font>]
	<strong>Method and apparatus for fast positioning using global positioning system</strong>
	<br><i><u>S Yoon</u>, S Kim</i>
	<br>KR 10-1564938, Oct. 27, 2015 &nbsp
    <p>
  </li>
<!--
  <li>
	<strong>Method and apparatus for changing personal information</strong>
	<br><i><u>S Yoon</u>, S Lee, O Kwon, M Kim, H Oh, Y Won, S Cha</i>
	<br>KR 1020130122222, Oct. 14, 2013
    <p>
  </li>
  <li>
	<strong>Apparatus and method for setting manner mode in portable terminal</strong>
	<br><i>J Nam, M Lee, M Koo, <u>S Yoon</u></i>
	<br>KR 1020110113927, Nov. 3, 2011
    <p>
  </li>
-->

</font>
</ol>

<!-- [<a href="{{ site.baseurl }}/assets/patent/patent_full.html">see all</a>] -->
