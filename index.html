---
layout: page
title: "David Seunghyun Yoon"
---

<img src="{{ site.baseurl }}/assets/profile/img" align="right" title="Profile Picture" class="profile">
<!--<font size="5"><strong>David Seunghyun Yoon </strong></font> <br>-->
<br>
<font size="3">
	Research Scientist<br>
    <a href="https://research.adobe.com/">Adobe Research</a>, San Jose, CA, US<br><br>
    <!--
    <strong>NLP Research Scientist</strong><br>
	Adobe Research (San Jose, CA, US)
    -->
</font>



<br>
<br style="line-height:1.0em"> <a href="mailto:syoon@adobe.com">mysmilesh@gmail.com</a>
<br>[<a href="{{ site.baseurl }}/assets/cv.pdf">CV</a>]&nbsp;
    [<a href="https://scholar.google.co.kr/citations?hl=en&user=UpymOMwAAAAJ&view_op=list_works&sortby=pubdate">Google Scholar</a>]&nbsp;
    [<a href="http://github.com/david-yoon/">GitHub</a>]&nbsp;
    [<a href="http://www.linkedin.com/in/david-s-yoon/">LinkedIn</a>]&nbsp;
    [<a href="http://twitter.com/david_s_yoon/">twitter</a>]
<br>

<br><br>







<hr>
<p>
I am a Research Scientist at Adobe Research. My research interests are in the areas of <strong>machine learning</strong> and <strong>natural language processing (NLP)</strong>. I am particularly interested in understanding long texts for question answering systems and learning language representation for NLP tasks. Further interests lie in applying and integrating NLP research with other disciplines to tackle practical issues; understanding multimodal information (i.e., text, audio, and visual) and NLP for social good.
    
    
<p> I received my Ph.D. in Electrical and Computer Engineering from Seoul National University in 2020 with the Distinguished Dissertation Award, where I was fortunate to be advised by <a href="http://milab.snu.ac.kr/kjung/index.html" title="Kyomin Jung">Dr. Kyomin Jung</a>.    
Prior to Seoul National University, I had involved critical initiatives for the engineering and innovation of AI and machine learning while I was a staff software engineer at Samsung Research Artificial Intelligence Center (2006-2017).


<!--
<p>I have published studies at NLP, AI, or signal processing conferences, such as <strong>ACL</strong>, <strong>NAACL</strong>, <strong>AAAI</strong>, <strong>CIKM</strong>, and <strong>ICASSP</strong>.
</p>

<p>
    My research interests are in the areas of <strong>machine learning</strong> and <strong>natural language processing (NLP)</strong>. I am particularly interested in understanding long texts for question answering systems and learning language representation for NLP tasks. Further interests lie in applying and integrating NLP research with other disciplines to tackle practical issues; understanding multimodal information (i.e., text, audio, and visual) and NLP for social good.

<p>My  Ph.D. advisor is <a href="http://milab.snu.ac.kr/kjung/index.html" title="Kyomin Jung">Dr. Kyomin Jung</a>, Dept. of Electrical and Computer Engineering, Seoul National University. Before coming to SNU, I was a staff software engineer at Samsung Research AI Center.
    
I have published studies at NLP, AI, or signal processing conferences, such as <strong>ACL</strong>, <strong>NAACL</strong>, <strong>AAAI</strong>, <strong>CIKM</strong>, and <strong>ICASSP</strong>.
</p>
-->


<!--<p><strong>Keywords:</strong> NLP, Machine Learning, Artificial Intelligence</p>-->
<hr>

<h3>News</h3>

<ul style="line-height:1.4em">
  <font size="2">
  <li>
	<font color=red>*new*</font>
	[01/2024]
	One paper (Textual Representation) is accepted to <a href="https://2024.eacl.org/">EACL 2024 Findings</a>.
 </li>
 <li>
	<font color=red>*new*</font>
	[11/2023]
	One paper (Video Topic Segmentation) is accepted to <a href="https://mmm2024.org/">Multimedia Modelling 2024</a>.
  </li>
 <li>
	[10/2023]
	One paper (Transcript Understanding) is accepted to <a href="https://bigdataieee.org/BigData2023/">IEEE BigData 2023</a>.
  </li>
 <li>	
	[10/2023]
	One paper (Image Captioining Metric) is accepted to <a href="https://2023.emnlp.org/">EMNLP 2023 Findings</a>.
  </li>
 <li>
	One paper (Moment Detection) is accepted to <a href="https://iccv2023.thecvf.com/">ICCV 2023</a>.
  </li>
 <li>
	[05/2023]
	One paper (Transcript Understanding) is accepted to <a href="https://www.interspeech2023.org/">Interspeech 2023</a>.
  </li>
 <li>
	[05/2023]
	Two papers (HighGEN, MeetingQA) are accepted to <a href="https://2023.aclweb.org/">ACL 2023</a>.
  </li>
 <li>
	[01/2023]
	One paper is accepted to <a href="https://2023.eacl.org/">EACL 2023</a>.
  </li>
</ul>

<a href="{{ site.baseurl }}/news.html">history</a>    
<hr>

<h3>Academic Activities</h3>
<ul style="line-height:1.4em">
  <li>
	  <strong><font color=darkblue>Service: </font></strong> <br>
	  <strong>Program Committee</strong>, NAACL (since 2019), ACL (since 2020), EMNLP (since 2019), AACL (since 2020), EACL (since 2021), COLING (since 2022), LREC (since 2023), ARR (since 2022) <br>
    <strong>Program Committee</strong>, AAAI (since 2020), WWW (since 2021), INTERSPEECH (2019), ICLR (since 2023) <br>
    <strong>Journal Reviewer</strong>, Information Processing and Management, 2020 <br>
    <strong>Journal Reviewer</strong>, IEEE Signal Processing Letters, 2020 <br>
    <br>
  </li>
  <li>
	<strong><font color=darkblue>Invited Talks: </font></strong> <br>
    Pretrained Language Model and Semantic Textual Understanding, <strong>SKKU</strong>, Sep. 2022<br>
    Semantic Textual Understanding for Information Retrieval, <a href="http://capp.snu.ac.kr/?p=workshop#program">Seoul National Univ.</a>, Aug. 2022<br>
    Mutimodal Evaluation Metric and Image Captioning Model, <strong>Korea Univ.</strong>, Dec. 2021<br>
    Recent Advancements in NLP for QA, LM, and Evaluation Metric, <strong>Dongguk Univ.</strong>, Sep. 2020<br>
    Understanding Long Texts for Question Answering System Using DNN, <strong>KAIST/IBS</strong>, Jul. 2020<br>
    Question Answering System for Long Text, <strong>Adobe Research</strong> (San Jose, CA, US), Dec. 2019<br>
    Question Answering System and Multimodal Speech Emotion Recognition, <strong>DEEPEST</strong>, Aug. 2019<br>
	Research in Natural Language Processing, 
	<a href="https://www.nvidia.com/ko-kr/ai-conference/"><strong>NVIDIA AI Conference</strong></a>, Jul. 2019<br>
	Question Answering for Short Answer, <strong>Adobe Research</strong> (San Jose, CA, US), Dec. 2018<br>
	QA-pair ranking algorithm and its applications, <strong>NAVER</strong>, Aug. 2018<br>
    Learning to Rank Question-Answer Pairs, <strong>PyTorch KR</strong>, Jun. 2018<br>
	Advancement of the Neural Dialogue Model, 
	<a href="https://www.fastcampus.co.kr/data_camp_lab/"><strong>Fast campus</strong></a>, 
	Jul. 2018 <br>
	<br>
  </li>
  <li><font size="2">
	<strong><font color=darkblue>Teaching Assistant: </font></strong> <br>
	Programming Methodology, Seoul National University, Spring 2018 <br>
	Machine Learning, Seoul National University, Fall 2015 <br>
	Lab. Sentiment Analysis, BigCamp (Big Data Academy), Big Data Institute, 2016-2019 <br>
	<br>
  </li>
  </font>
</ul>


<hr>

<h3>Professional Experiences</h3>
<ul style="line-height:1.4em">
	<li><font size="2">
	<strong><font color=darkblue>NLP Research Scientist: </font></strong>
	Adobe Research (San Jose, CA, US),
	2020-present
  </li>
  <li><font size="2">
	<strong><font color=darkblue>Research Scientist Intern: </font></strong>
	Adobe Research (San Jose, CA, US),
	Fall 2018 
  </li>
  <li>
	<strong><font color=darkblue>Staff Engineer: </font></strong>
	Samsung Research (Seoul, KR),
	2006-2017
  </li>
  <li>
	<strong><font color=darkblue>Representative of employees: </font></strong>
	Samsung Electronics (Seoul, KR),
	2012-2014
  </li>
  <li>
	<strong><font color=darkblue>Trainer of Global New Employee Course: </font></strong>
	Samsung Electronics (Seoul, KR),
	Spring 2011 
  </li></font></ul>
</ol>
<hr>
<h3>Publications</h3>
<!--*denotes equal contribution.<br>-->
<ol style="line-height:1.4em" reversed>
  <font size="2">
  <h4><strong>[arXiv]</strong></h4>
	<li>
		<strong>PDFTriage: Question Answering over Long, Structured Documents</strong>
		<a href="https://arxiv.org/pdf/2309.08872.pdf">[arxiv]</a>
		<br><i>Jon Saad-Falcon, Joe Barrow, Alexa Siu, Ani Nenkova, <u>Seunghyun Yoon</u>, Ryan A. Rossi, Franck Dernoncourt</i>
		<br><a href="nan">arxiv</a>
		<p>
	</li>
	<li>
		<strong>Multilingual Sentence-Level Semantic Search using Meta-Distillation Learning</strong>
		<a href="https://arxiv.org/pdf/2309.08185.pdf">[arxiv]</a>
		<br><i>Meryem M'hamdi, Jonathan May, Franck Dernoncourt, Trung Bui, <u>Seunghyun Yoon</u></i>
		<br><a href="nan">arxiv</a>
		<p>
	</li>
	<li>
		<strong>MVMR: Evaluating Natural Language Video Localization Bias over Multiple Reliable Videos Pool</strong>
		<a href="https://arxiv.org/pdf/2309.16701.pdf">[arxiv]</a>
		<br><i>Nakyeong Yang, Minsung Kim, <u>Seunghyun Yoon</u>, Joongbo Shin, Kyomin Jung</i>
		<br><a href="nan">arxiv</a>
		<p>
	</li>
  <h4><strong>[2024]</strong></h4>
	<li>
		<strong>Fine-tuning CLIP Text Encoders with Two-step Paraphrasing</strong>
		<br><i>Hyunjae Kim, <u>Seunghyun Yoon</u>, Trung Bui, Handong Zhao, Quan Hung Tran, Franck Dernoncourt, Jaewoo Kang</i>
		<br><a href="https://2024.eacl.org/">EACL 2024 Findings</a>
		<p>
	</li>
	<li>
		<strong>Multi-Modal Video Topic Segmentation with Dual-Contrastive Domain Adaptation</strong>
		<br><i>Linzi Xing, Quan Hung Tran, Fabian Caba Heilbron, Franck Dernoncourt, <u>Seunghyun Yoon</u>, Zhaowen Wang, Trung Bui, Giuseppe Carenini</i>
		<br><a href="https://mmm2024.org/">Multimedia Modelling 2024</a>
		<p>
	</li>
  <h4><strong>[2023]</strong></h4>
	<li>
		<strong>Aspect-based Meeting Transcript Summarization: A Two-Stage Approach with Weak Supervision on Sentence Classification</strong>
		<a href="https://arxiv.org/pdf/2311.04292.pdf">[pdf]</a>
		<br><i>Zhongfen Deng, <u>Seunghyun Yoon</u>, Trung Bui, Franck Dernoncourt, Quan Hung Tran, Shuaiqi Liu, Wenting Zhao, Tao Zhang, Yibo Wang, Philip Yu</i>
		<br><a href="https://bigdataieee.org/BigData2023/">IEEE BigData 2023</a>
		<p>
	</li>
	<li>
		<strong>Perturbation Robust Metric for Multi-Lingual Image Captioning</strong>
		<a href="https://aclanthology.org/2023.findings-emnlp.819.pdf">[pdf]</a>
		<br><i>Yongil Kim, Yerin Hwang, Hyeongu Yun, <u>Seunghyun Yoon</u>, Trung Bui, Kyomin Jung</i>
		<br><a href="https://2023.emnlp.org/">EMNLP 2023 Findings</a>
		<p>
	</li>
	<li>
		<strong>Moment Detection in Long Tutorial Videos</strong>
		<a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Croitoru_Moment_Detection_in_Long_Tutorial_Videos_ICCV_2023_paper.pdf">[pdf]</a>
		<a href="https://github.com/ioanacroi/longmoment-detr">[code]</a>
		<br><i>Ioana Croitoru, Simion-Vlad Bogolin, Samuel Albanie, Yang Liu, Zhaowen Wang, <u>Seunghyun Yoon</u>, Franck Dernoncourt, Hailin Jin, Trung Bui</i>
		<br><a href="https://iccv2023.thecvf.com/">ICCV 2023</a>
		<p>
	</li>
	<li>
		<strong>Boosting Punctuation Restoration with Data Generation and Reinforcement Learning</strong>
		<a href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/lai23c_interspeech.pdf">[pdf]</a>
		<br><i>Viet Lai, Abel Salinas, Hao Tan, Trung Bui, Quan Hung Tran, <u>Seunghyun Yoon</u>, Hanieh Deilamsalehy, Franck Dernoncourt, Thien Nguyen</i>
		<br><a href="https://www.interspeech2023.org/">Interspeech 2023</a>
		<p>
	</li>
	<li>
		<strong>Automatic Creation of Named Entity Recognition Datasets by Querying Phrase Representations</strong>
		<a href="https://aclanthology.org/2023.acl-long.394.pdf">[pdf]</a>
		<br><i>Hhynjae Kim, Jaehyo Yoo, <u>Seunghyun Yoon</u>, Jaewoo Kang</i>
		<br><a href="https://2023.aclweb.org/">ACL 2023</a>
		<p>
	</li>
	<li>
		<strong>MEETINGQA: Extractive Question-Answering on Meeting Transcripts</strong>
		<a href="https://aclanthology.org/2023.acl-long.837.pdf">[pdf]</a>
		<br><i>Archiki Prasad, Trung Bui, <u>Seunghyun Yoon</u>, Hanieh Deilamsalehy, Franck Dernoncourt, Mohit Bansal</i>
		<br><a href="https://2023.aclweb.org/">ACL 2023</a>
		<p>
	</li>
	<li>
		<strong>PiC: A Phrase-in-Context Dataset for Phrase Understanding and Semantic Search</strong>
		<a href="https://aclanthology.org/2023.eacl-main.1.pdf">[pdf]</a>
		<a href="https://phrase-in-context.github.io/">[page]</a>
		<br><i>Thang M. Pham, <u>Seunghyun Yoon</u>, Trung Bu, Anh Nguyeng</i>
		<br><a href="https://2023.eacl.org/">EACL 2023</a>
		<p>
	</li>
  <h4><strong>[2022]</strong></h4>
	<li>
		<strong>Factual Error Correction for Abstractive Summaries Using Entity Retrieval</strong>
		<a href="https://aclanthology.org/2022.gem-1.41.pdf">[pdf]</a>
		<br><i>Hwanhee Lee, Cheoneum Park, <u>Seunghyun Yoon</u>, Trung Bu, Franck Dernoncourt, Juae Kim, Kyomin Jung</i>
		<br><a href="https://2022.emnlp.org/">EMNLP 2022 Workshop on GEM</a>
		<p>
	</li>
	<li>
		<strong>Improving cross-modal attention via object detection</strong>
		<a href="https://attention-learning-workshop.github.io/2022/papers/kim-improving_crossmodal_attention_via_object_detection.pdf">[pdf]</a>
		<br><i>Yongil Kim, Yerin Hwang, <u>Seunghyun Yoon</u>, Hyeongu Yun, Kyomin Jung</i>
		<br><a href="https://nips.cc/">NeurIPS 2022 Workshop on All Things Attention</a>
		<p>
	</li>
	<li>
		<strong>Simple Questions Generate Named Entity Recognition Datasets</strong>
		<a href="https://aclanthology.org/2022.emnlp-main.417.pdf">[pdf]</a>
		<a href="https://github.com/dmis-lab/GeNER">[code]</a>
		<br><i>Hyunjae Kim, Jaehyo Yoo, <u>Seunghyun Yoon</u>, Jinhyuk Lee, Jaewoo Kang</i>
		<br><a href="https://2022.emnlp.org/">EMNLP 2022</a>
		<p>
	</li>
	<li>
		<strong>Virtual Knowledge Graph Construction for Zero-Shot Domain-Specific Document Retrieval</strong>
		<a href="https://aclanthology.org/2022.coling-1.101.pdf">[pdf]</a>
		<br><i>Yeon Seonwoo, <u>Seunghyun Yoon</u>, Franck Dernoncourt, Trung Bui, Alice Oh</i>
		<br><a href="https://coling2022.org/">COLING 2022</a>
		<p>
	</li>
	<li>
		<strong>Medical Question Understanding and Answering with Knowledge Grounding and Semantic Self-Supervision</strong>
		<a href="https://aclanthology.org/2022.coling-1.241.pdf">[pdf]</a>
		<br><i>Khalil Mrini, Harpreet Singh, Franck Dernoncourt, <u>Seunghyun Yoon</u>, Trung Bui, Walter W. Chang, Emilia Farcas, Ndapa Nakashole</i>
		<br><a href="https://coling2022.org/">COLING 2022</a>
		<p>
	</li>
	<li>
		<strong>Offensive Content Detection Via Synthetic Code-Switched Text</strong>
		<a href="https://aclanthology.org/2022.coling-1.575.pdf">[pdf]</a>
		<br><i>Cesa Salaam, Franck Dernoncourt, Trung Bui, <u>Seunghyun Yoon</u></i>
		<br><a href="https://coling2022.org/">COLING 2022</a>
		<p>
	</li>
	<li>
		<strong>Keyphrase Prediction from Video Transcripts: New Dataset and Directions</strong>
		<a href="https://aclanthology.org/2022.coling-1.624.pdf">[pdf]</a>
		<br><i>Amir Pouran Ben Veyseh, Quan Hung Tran, <u>Seunghyun Yoon</u>, Varun Manjunatha, Hanieh Deilamsalehy, Rajiv Jain, Trung Bui, Walter W. Chang, Franck Dernoncourt, Thien Huu Nguyen</i>
		<br><a href="https://coling2022.org/">COLING 2022</a>
		<p>
	</li>
	<li>
		<strong>MACRONYM: A Large-Scale Dataset for Multilingual and Multi-Domain Acronym Extraction </strong>
		<a href="https://aclanthology.org/2022.coling-1.292.pdf">[pdf]</a>
		<br><i>Amir Pouran Ben Veyseh, Nicole Meister, <u>Seunghyun Yoon</u>, Rajiv Jain, Franck Dernoncourt, Thien Huu Nguyen</i>
		<br><a href="https://coling2022.org/">COLING 2022</a>
		<p>
	</li>
	<li>
		<strong>Fine-grained Image Captioning with CLIP Reward</strong>
		<a href="https://aclanthology.org/2022.findings-naacl.39.pdf">[pdf]</a>
		<a href="https://github.com/j-min/CLIP-Caption-Reward">[code]</a>
		<a href="https://huggingface.co/spaces/NAACL2022/CLIP-Caption-Reward">[demo]</a>
		<br><i>Jaemin Cho, <u>Seunghyun Yoon</u>, Ajinkya Kale, Franck Dernoncourt, Trung Bui, M Bansal</i>
		<br><a href="https://2022.naacl.org/">NAACL 2022 Findings</a>
		<p>
	</li>
	<li>
		<strong>Multimodal Intent Discovery from Livestream Videos</strong>
		<a href="https://aclanthology.org/2022.findings-naacl.36.pdf">[pdf]</a>
		<a href="https://github.com/adymaharana/VideoIntentDiscovery">[code]</a>
		<br><i>Adyasha Maharana, Quan Tran, <u>Seunghyun Yoon</u>, Franck Dernoncourt, Trung Bui, Walter Chang, M Bansal</i>
		<br><a href="https://2022.naacl.org/">NAACL Findings 2022</a>
		<p>
	</li>
	<li>
		<strong>How does fake news use a thumbnail? CLIP-based Multimodal Detection on the Unrepresentative News Image</strong>
		<a href="https://aclanthology.org/2022.constraint-1.10.pdf">[pdf]</a>
		<br><i>Hyewon Choi, Yejun Yoon, <u>Seunghyun Yoon</u>, Kunwoo Park</i>
		<br><a href="https://lcs2.iiitd.edu.in/CONSTRAINT-2022/">ACL CONSTRAINT 2022</a>
		<p>
	</li>
	<li>
		<strong>CAISE: Conversational Agent for Image Search and Editing</strong>
		<a href="https://arxiv.org/abs/2202.11847">[pdf]</a>
		<a href="https://github.com/hyounghk/CAISE">[code]</a>
		<br><i>Hyounghun Kim, Doo Soon Kim, <u>Seunghyun Yoon</u>, Franck Dernoncourt, Trung Bui, Mohit Bansal</i>
		<br><a href="https://aaai.org/Conferences/AAAI-22/">AAAI 2022</a>
		<p>
	</li>
  <h4><strong>[2021]</strong></h4>
	<li>
		<strong>Few-Shot Intent Detection via Contrastive Pre-Training and Fine-Tuning</strong>
		<a href="https://arxiv.org/abs/2109.06349">[pdf]</a>
		<br><i>J Zhang, T Bui, <u>S Yoon</u>, X Chen, Z Liu, C Xia, QH Tran, W Chang, P Yue</i>
		<br><a href="https://2021.emnlp.org/">EMNLP 2021</a>
		<p>
	</li>
	<li>
		<strong>QACE: Asking Questions to Evaluate an Image Caption</strong>
		<a href="https://arxiv.org/abs/2108.12560">[pdf]</a>
		<a href="https://github.com/hwanheelee1993/QACE">[code]</a>
		<br><i>H Lee, T Scialom, <u>S Yoon</u>, F Dernoncourt, K Jung</i>
		<br><a href="https://2021.emnlp.org/">EMNLP 2021 Findings</a>
		<p>
	</li>
	<li>
		<strong>A Gradually Soft Multi-Task and Data-Augmented Approach to Medical Question Understanding</strong>
		<a href="https://aclanthology.org/2021.acl-long.119.pdf">[pdf]</a>
		<br><i>K Mrini, F Dernoncourt, <u>S Yoon</u>, T Bui, W Chang, E Farcas, N Nakashole</i>
		<br><a href="https://2021.aclweb.org/">ACL 2021</a>
		<p>
	</li>
	<li>
		<strong>UMIC: An Unreferenced Metric for Image Captioning via Contrastive Learning</strong>
		<a href="https://aclanthology.org/2021.acl-short.29.pdf">[pdf]</a>
		<a href="https://github.com/hwanheelee1993/UMIC">[code]</a>
		<br><i>H Lee, <u>S Yoon</u>, F Dernoncourt, T Bui, K Jung</i>
		<br><a href="https://2021.aclweb.org/">ACL 2021</a>
		<p>
	</li>
	<li>
		<strong>UCSD-Adobe at MEDIQA 2021: Transfer Learning and Answer Sentence Selection for Medical Summarization</strong>
		<a href="https://www.aclweb.org/anthology/2021.bionlp-1.28.pdf">[pdf]</a>
		<br><i>K Mrini, F Dernoncourt, <u>S Yoon</u>, T Bui, W Chang, E Farcas, N Nakashole</i>
		<br><a href="https://aclweb.org/aclwiki/BioNLP_Workshop">NAACL BioNLP 2021</a>
		<p>
	</li>
	<li>
		<strong>KPQA: A Metric for Generative Question Answering Using Keyphrase Weights</strong>
		<a href="https://www.aclweb.org/anthology/2021.naacl-main.170.pdf">[pdf]</a>
		<a href="https://github.com/hwanheelee1993/KPQA">[code]</a>
		<br><i>H Lee, <u>S Yoon</u>, F Dernoncourt, DS Kim, T Bui, J Shin, K Jung</i>
		<br><a href="https://2021.naacl.org/">NAACL 2021</a>
		<p>
	</li>
	<li>
		<strong>Learning to Detect Incongruence in News Headline and Body Text via a Graph Neural Network</strong>
		<a href="https://ieeexplore.ieee.org/document/9363185">[pdf]</a>
		<a href="https://github.com/minwhoo/detecting-incongruity-gnn">[code]</a>
		<br><font color=orange>(SCI, IF=3.745)</font>
		<br><i><u>S Yoon</u>*, K Park*, M Lee, T Kim, M Cha, K Jung</i>
		<br><a href="https://ieeeaccess.ieee.org/">IEEE Access 2021</a>
		<p>
	</li>
  <h4><strong>[2020]</strong></h4>
	<li>
		<strong>Collaborative Training of GANs in Continuous and Discrete Spaces for Text Generation</strong>
		<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9296209">[pdf]</a>
		<br><font color=orange>(SCI, IF=3.745)</font>
		<br><i>Y Kim, S Won, <u>S Yoon</u>, K Jung</i>
		<br><a href="https://ieeeaccess.ieee.org/">IEEE Access 2020</a>
		<p>
	</li>
	<li>
		<strong>ViLBERTScore: Evaluating Image Caption Using Vision-and-Language BERT</strong>
		<a href="https://www.aclweb.org/anthology/2020.eval4nlp-1.4.pdf">[pdf]</a>
		<a href="https://github.com/hwanheelee1993/ViLBERTScore">[code]</a>
		<br><i>H Lee, <u>S Yoon</u>, F Dernoncourt, DS Kim, T Bui, K Jung</i>
		<br><a href="https://nlpevaluation2020.github.io/index.html">EMNLP Eval4NLP 2020</a>
		<p>
	</li>
	<li>
		<strong>Multimodal Speech Emotion Recognition using Cross Attention with Aligned Audio and Text</strong>
		<a href="https://arxiv.org/pdf/2207.12895.pdf">[pdf]</a>
		<br><i>Y Lee, <u>S Yoon</u>, K Jung</i>
		<br><a href="https://arxiv.org/abs/2207.12895/">INTERSPEECH 2020</a>
		<p>
	</li>
	<li>
		<strong>Fast and Accurate Deep Bidirectional Language Representations for Unsupervised Learning</strong>
		<a href="https://www.aclweb.org/anthology/2020.acl-main.76.pdf">[pdf]</a>
		<br><i>J Shin, Y Lee, <u>S Yoon</u>, K Jung</i>
		<br><a href="https://acl2020.org/">ACL 2020</a>
		<p>
	</li>
	<li>
		<strong>Propagate-Selector: Detecting Supporting Sentences for Question Answering via Graph Neural Networks</strong>
		<a href="https://www.aclweb.org/anthology/2020.lrec-1.664">[pdf]</a>
		<a href="https://github.com/david-yoon/propagate-selector">[code]</a>
		<br><i><u>S Yoon</u>, F Dernoncourt, DS Kim, T Bui, K Jung</i>
		<br><a href="https://lrec2020.lrec-conf.org/">LREC 2020</a>
		<p>
	</li>
	<li>
		<strong>Drug-disease Graph: Predicting Adverse Drug Reaction Signals via Graph Neural Network with Clinical Data</strong>
		<a href="https://arxiv.org/pdf/2004.00407.pdf">[pdf]</a>
		<a href="https://pakdd2020.org/download/conference_paper_slides/main-851.pdf">[slide]</a>
		<br><font color=orange>(oral presentation)</font>
		<br><i>H Kwak, M Lee, <u>S Yoon</u>, J Chang, S Park, K Jung</i>
		<br><a href="https://pakdd2020.org/">PAKDD 2020</a>
		<p>
	</li>
	<li>
		<strong>DSTC8-AVSD: Multimodal Semantic Transformer Network with Retrieval Style Word Generator</strong>
		<a href="https://arxiv.org/pdf/2004.08299.pdf">[pdf]</a>
		<br><i>H Lee, <u>S Yoon</u>, F Dernoncourt, DS Kim, T Bui, K Jung</i>
		<br><a href="https://aaai.org/Conferences/AAAI-20/ws20workshops/#ws09">AAAI 2020 DSTC8</a>
		<p>
	</li>
	<li>
		<strong>Comparative Studies on Machine Learning for Paralinguistic Signal Compression and Classification</strong>
		<a href="https://link.springer.com/content/pdf/10.1007/s11227-020-03346-3.pdf">[pdf]</a>
		<br><font color=orange>(SCI, IF=2.157)</font>
		<br><i>S Byun*, <u>S Yoon</u>*, K Jung</i>
		<br><a href="https://www.springer.com/journal/11227/?gclid=EAIaIQobChMIrJqOzti96QIVBhdgCh1UVAn_EAAYASAAEgLwp_D_BwE/">Journal of Supercomputing 2020</a>
		<p>
	</li>
	<li>
		<strong>Attentive Modality Hopping Mechanism for Speech Emotion Recognition</strong>
		<a href="https://david-yoon.github.io/assets/paper/yoon2020attentive.pdf">[pdf]</a>
		<a href="https://github.com/david-yoon/attentive-modality-hopping-for-SER">[code]</a>
		<a href="https://www.slideshare.net/DavidSeunghyunYoon/slide-attentive-modality-hopping-mechanism-for-speech-emotion-recognition">[slide]</a>
		<br><font color=orange>(oral presentation)</font>
		<br><i><u>S Yoon</u>, S Dey, H Lee, K Jung</i>
		<br><a href="https://2020.ieeeicassp.org/">IEEE ICASSP 2020</a>
		<p>
	</li>
	<li>
		<strong>BaitWatcher: A lightweight web interface for the detection of incongruent news headlines</strong>
		<a href="https://arxiv.org/pdf/2003.11459.pdf">[pdf]</a>
		<a href="https://www.springer.com/gp/book/9783030426989">[book]</a>
		<br><i>K Park, T Kim, <u>S Yoon</u>, M Cha, K Jung</i>
		<br><a href="https://www.springer.com/gp/book/9783030426989">Disinformation, Misinformation, and Fake News in Social Media-Emerging Research Challenges and Opportunities, Springer 2020</a>
		<p>
	</li>
  <h4><strong>[2019]</strong></h4>
	<li>
		<strong>A Compare-Aggregate Model with Latent Clustering for Answer Selection</strong>
		<a href="https://arxiv.org/pdf/1905.12897.pdf">[pdf]</a>
		<a href="https://www.slideshare.net/DavidSeunghyunYoon/slide-a-compareaggregate-model-with-latent-clustering-for-answer-selection">[slide]</a>
		<a href="https://www.slideshare.net/DavidSeunghyunYoon/poster-a-compareaggregate-model-with-latent-clustering-for-answer-selection">[poster]</a>
		<br><font color=orange>(oral presentation)</font>
		<br><i><u>S Yoon</u>, F Dernoncourt, DS Kim, T Bui, K Jung</i>
		<br><a href="http://www.cikm2019.net/">CIKM 2019</a>
		<p>
	</li>
	<li>
		<strong>Surf at MEDIQA 2019: Improving Performance of Natural Language Inference in the Clinical Domain by Adopting Pre-trained Language Model</strong>
		<a href="https://www.aclweb.org/anthology/W19-5043.pdf">[pdf]</a>
		<br><i>J Nam, <u>S Yoon</u>, K Jung</i>
		<br><a href="https://aclweb.org/aclwiki/BioNLP_Workshop">ACL BioNLP 2019</a>
		<p>
	</li>
	<li>
		<strong>Speech Emotion Recognition Using Multi-hop Attention Mechanism</strong>
		<a href="https://arxiv.org/pdf/1904.10788.pdf">[pdf]</a>
		<a href="https://sigport.org/documents/speech-emotion-recognition-using-multi-hop-attention-mechanism">[slide]</a>
		<br><font color=orange>(oral presentation)</font>
		<br><i><u>S Yoon</u>, S Byun, S Dey, K Jung</i>
		<br><a href="https://2019.ieeeicassp.org/">IEEE ICASSP 2019</a>
		<p>
	</li>
	<li>
		<strong>Neural Networks for Compressing and Classifying Speaker-Independent Paralinguistic Signals</strong>
		<a href="http://milab.snu.ac.kr/pub/BigComp2019Byun.pdf">[pdf]</a>
		<br><i>S Byun, <u>S Yoon</u>, K Jung</i>
		<br><a href="http://www.bigcomputing.org/">IEEE BigComp 2019</a>
		<p>
	</li>
	<li>
		<strong>Detecting Incongruity Between News Headline and Body Text via a Deep Hierarchical Encoder </strong>
		<a href="https://wvvw.aaai.org/ojs/index.php/AAAI/article/view/3756/3634">[pdf]</a>
		<a href="https://github.com/david-yoon/detecting-incongruity">[code]</a>
		<br><font color=orange>(oral presentation)</font>
		<br><i><u>S Yoon</u>*, K Park*, J Shin, H Lim, S Won, M Cha, K Jung</i>
		<br><a href="https://aaai.org/Conferences/AAAI-19/">AAAI 2019</a>
		<p>
	</li>
  <h4><strong>[2018 and earlier]</strong></h4>
	<li>
		<strong>Multimodal Speech Emotion Recognition using Audio and Text </strong>
		<a href="https://arxiv.org/pdf/1810.04635.pdf">[pdf]</a>
		<a href="https://github.com/david-yoon/multimodal-speech-emotion">[code]</a>
		<br><i><u>S Yoon</u>, S Byun, K Jung</i>
		<br><a href="http://www.slt2018.org/">IEEE SLT 2018</a>
		<p>
	</li>
	<li>
		<strong>Comparative Studies of Detecting Abusive Language on Twitter</strong>
		<a href="https://www.aclweb.org/anthology/W18-5113.pdf">[pdf]</a>
		<a href="https://github.com/younggns/comparative-abusive-lang">[code]</a>
		<br><i>Y Lee*, <u>S Yoon</u>*, K Jung</i>
		<br><a href="https://sites.google.com/view/alw2018">EMNLP ALW 2018</a>
		<p>
	</li>
	<li>
		<strong>Learning to Rank Question-Answer Pairs using Hierarchical Recurrent Encoder with Latent Topic Clustering</strong>
		<a href="https://www.aclweb.org/anthology/N18-1142.pdf">[pdf]</a>
		<a href="https://github.com/david-yoon/QA_HRDE_LTC">[code]</a>
		<br><i><u>S Yoon</u>, J Shin, K Jung</i>
		<br><a href="http://naacl2018.org/">NAACL 2018</a>
		<p>
	</li>
	<li>
		<strong>Contextual-CNN: A Novel Architecture Capturing Unified Meaning for Sentence Classification</strong>
		<a href="http://milab.snu.ac.kr/pub/BigComp2018.pdf">[pdf]</a>
		<br><i>J Shin, Y Kim, <u>S Yoon</u>, K Jung</i>
		<br><a href="http://www.bigcomputing.org/">IEEE BigComp 2018</a>
		<p>
	</li>
	<li>
		<strong>Synonym Discovery with Etymology-based Word Embeddings</strong>
		<a href="https://arxiv.org/pdf/1709.10445.pdf">[pdf]</a>
		<br><i><u>S Yoon</u>, P Estrada, K Jung</i>
		<br><a href="http://www.ele.uri.edu/ieee-ssci2017/">IEEE SSCI 2017</a>
		<p>
	</li>
	<li>
		<strong>Efficient Transfer Learning Schemes for Personalized Language Modeling using Recurrent Neural Network</strong>
		<a href="http://milab.snu.ac.kr/pub/AAAI2017Yoon.pdf">[pdf]</a>
		<br><i><u>S Yoon</u>, H Yun, Y Kim, G Park, K Jung</i>
		<br><a href="http://crowdai.azurewebsites.net/">AAAI 2017 (Workshop)</a>
		<p>
	</li>
	<li>
		<strong>Automatic Question Answering System for Consumer Product</strong>
		<a href="http://milab.snu.ac.kr/pub/IntelliSys2016Yoon.pdf">[pdf]</a>
		<br><i><u>S Yoon</u>, M Sundar, A Gupta, K Jung</i>
		<br><a href="http://saiconference.com/Conferences/IntelliSys2016%22">IntelliSys 2016</a>
		<p>
	</li>
	<li>
		<strong>Mining the Minds of Customers from Online Chat Logs</strong>
		<a href="https://arxiv.org/abs/1510.01801">[pdf]</a>
		<br><i>K Park, J Kim, J Park, M Cha, J Nam, <u>S Yoon</u>, E Rhim</i>
		<br><a href="http://www.cikm-2015.org/">CIKM 2015</a>
		<p>
	</li>
	<li>
		<strong>Domain Question Answering System</strong>
		<a href="nan">[pdf]</a>
		<br><i><u>S Yoon</u>, E Rhim, D Kim</i>
		<br><a href="nan">KIISE Transactions on Computing Practices 2015</a>
		<p>
	</li>
	<li>
		<strong>Media clips: Implementation of an intuitive media linker</strong>
		<a href="https://david-yoon.github.io/assets/paper/MediaClip_BMSB2011.pdf">[pdf]</a>
		<br><i><u>S Yoon</u>, K Lee, H Shin</i>
		<br><a href="https://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp?punumber=5945017">IEEE BMSB 2011</a>
		<p>
	</li>
  </font>
</ol>
<hr>
<h3>Patents</h3>
<font color=lightblue>
  <h4>[  International Patents ]</h4>
</font>
<ol reversed style="line-height:1.4em"><font size="2">	<li>
		[<font color=magenta>issued</font>]		<strong>Utilizing a graph neural network to identify supporting text phrases and generate digital query responses </strong>
		<a href="https://patents.google.com/patent/US20210058345A1/en?oq=US20210058345A1">[link]</a>
		<br><i><u>S Yoon</u>, F Dernoncourt, DS Kim, T Bui</i>
		<br>US 11,271,876, 8-Mar-2022
		<p>
	</li>
	<li>
		[<font color=magenta>issued</font>]		<strong>Utilizing bi-directional recurrent encoders with multi-hop attention for speech emotion recognition</strong>
		<a href="https://patents.google.com/patent/US20210050033A1/en?oq=US20210050033A1">[link]</a>
		<br><i>T Bui, S Dey, <u>S Yoon</u></i>
		<br>US 11,205,444, 21-Dec-2021
		<p>
	</li>
	<li>
		[<font color=magenta>issued</font>]		<strong>Answer selection using a compare-aggregate model with language model and condensed similarity information from latent clustering</strong>
		<a href="https://patents.google.com/patent/US11113323B2/en?oq=US11113323B2">[link]</a>
		<br><i><u>S Yoon</u>, F Dernoncourt, T Bui, DS Kim, CI Dockhorn, Y Gong</i>
		<br>US 11,113,323, 7-Sep-2021
		<p>
	</li>
	<li>
		[<font color=magenta>issued</font>]		<strong>Terminal apparatus, server and method of controlling the same</strong>
		<a href="https://patents.google.com/patent/US10084850B2/en?oq=US+10%2c084%2c850/">[link]</a>
		<br><i>Y Kim, O Kwon, S Kim, H Oh, <u>S Yoon</u>, S Cha, J Lee</i>
		<br>US 10,084,850, CN 201410085759, EP20140154718, 25-Sep-2018
		<p>
	</li>
	<li>
		<strong>Method and device for analyzing user's emotion</strong>
		<a href="https://patentscope.wipo.int/search/en/detail.jsf?docId=WO2016182393">[link]</a>
		<br><i>E Rhim, J Kim, J Nam, <u>S Yoon</u>, K Park, J Park, M Cha</i>
		<br>WO2016182393, 13-May-2016
		<p>
	</li>
	<li>
		[<font color=magenta>issued</font>]		<strong>Method of recommending application, mobile terminal using the method, and communication system using the method </strong>
		<a href="https://patents.google.com/patent/US9247376B2/en?oq=US+9%2c247%2c376">[link]</a>
		<br><i>J Nam, M Lee, M Koo, <u>S Yoon</u></i>
		<br>US 9,247,376, 26-Jan-2016
		<p>
	</li>
	<li>
		[<font color=magenta>issued</font>]		<strong>Method and apparatus for displaying photo on screen having any shape</strong>
		<a href="https://patents.google.com/patent/US9049383B2/en?oq=US+9%2c049%2c383">[link]</a>
		<br><i><u>S Yoon</u>, M Lee</i>
		<br>US 9,049,383, 2-Jun-2015
		<p>
	</li>
	<li>
		[<font color=magenta>issued</font>]		<strong>Method and apparatus for providing information and computer readable storage medium having a program recorded thereon for executing the method </strong>
		<a href="https://patents.google.com/patent/US8958824B2/en?oq=US+8%2c958%2c824">[link]</a>
		<br><i><u>S Yoon</u>, M Lee, M Koo, J Nam</i>
		<br>US 8,958,824, 17-Feb-2015
		<p>
	</li>
	<li>
		[<font color=magenta>issued</font>]		<strong>Apparatus and method for clipping and sharing content at a portable terminal</strong>
		<a href="https://patents.google.com/patent/CN103827913A/en?oq=CN103827913A">[link]</a>
		<br><i><u>S Yoon</u>, M Lee, M Koo, J Nam</i>
		<br>US 13/629,394, CN103827913A, EP20120837007, PCT/KR1020110097578, 28-May-2014
		<p>
	</li>
	<li>
		[<font color=magenta>issued</font>]		<strong>Method and apparatus for fast tracking position by using global positioning system</strong>
		<a href="https://patents.google.com/patent/US8094070B2/en?oq=US8094070B2">[link]</a>
		<br><i><u>S Yoon</u>, S Kim</i>
		<br>US 8,094,070, 10-Jan-2012
		<p>
	</li>
  </font>
</ol>
<font color=lightblue>
  <h4>[ Korean Patents ]</h4>
</font>
<ol reversed style="line-height:1.4em"><font size="2">	<li>
		[<font color=magenta>issued</font>]		<strong>Apparatus and method for evaluating sentense by using bidirectional language model</strong>
		<br><i>K Jung, J Shin, <u>S Yoon</u></i>
		<br>KR 10-2436900, 23-Aug-2022
		<p>
	</li>
	<li>
		[<font color=magenta>issued</font>]		<strong>Method and apparatus for emotion recognition based on cross attentionmodel</strong>
		<br><i>K Jung, Y Lee, <u>S Yoon</u></i>
		<br>KR 10-2365433, 16-Feb-2022
		<p>
	</li>
	<li>
		[<font color=magenta>issued</font>]		<strong>Artificial intelligence based dialog system and response control method thereof</strong>
		<br><i>K Jung, <u>S Yoon</u>, J Shin, H Kwak, S Byun</i>
		<br>KR 10-2059015, 18-Dec-2019
		<p>
	</li>
	<li>
		[<font color=magenta>issued</font>]		<strong>Terminal apparatus, server and method of controlling the same</strong>
		<br><i>Y Kim, O Kwon, S Kim, H Oh, <u>S Yoon</u>, S Cha, J Lee</i>
		<br>KR 10-1832394, 20-Feb-2018
		<p>
	</li>
	<li>
		[<font color=magenta>issued</font>]		<strong>Apparatus and method for collecting information of destination in portable terminal</strong>
		<br><i><u>S Yoon</u>, J Nam, M Koo, M Lee</i>
		<br>KR 10-1914632, 29-Oct-2018
		<p>
	</li>
	<li>
		[<font color=magenta>issued</font>]		<strong>Method and apparatus for providing information, and computer readable storage medium</strong>
		<br><i><u>S Yoon</u>, M Lee, M Koo, J Nam</i>
		<br>KR 10-1773167, 24-Aug-2017
		<p>
	</li>
	<li>
		[<font color=magenta>issued</font>]		<strong>Method for recommendation of application, mobile terminal thereof and communication system thereof</strong>
		<br><i>J Nam, M Lee, M Koo, <u>S Yoon</u></i>
		<br>KR 10-1747303, 8-Jun-2017
		<p>
	</li>
	<li>
		<strong>Device and method for analyzing user emotion</strong>
		<br><i>E Rhim, J Kim, J Nam, <u>S Yoon</u>, K Park, J Park, M Cha</i>
		<br>KR 1020160058782, 13-May-2016
		<p>
	</li>
	<li>
		[<font color=magenta>issued</font>]		<strong>Method and apparatus for fast positioning using global positioning system</strong>
		<br><i><u>S Yoon</u>, S Kim</i>
		<br>KR 10-1564938, 27-Oct-2015
		<p>
	</li>
  </font>
</ol>
