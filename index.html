---
layout: page
title: "David Seunghyun Yoon"
---

<img src="{{ site.baseurl }}/assets/profile/img" align="right" title="Profile Picture" class="profile">
<!--<font size="5"><strong>David Seunghyun Yoon </strong></font> <br>-->
<br>
<font size="3">
	Research Scientist<br>
    <a href="https://research.adobe.com/">Adobe Research</a>, San Jose, CA, US<br><br>
    <!--
    <strong>NLP Research Scientist</strong><br>
	Adobe Research (San Jose, CA, US)
    -->
</font>



<br>
<br style="line-height:1.0em"> <a href="mailto:syoon@adobe.com">mysmilesh@gmail.com</a>
<br>[<a href="{{ site.baseurl }}/assets/cv.pdf">CV</a>]&nbsp;
    [<a href="https://scholar.google.co.kr/citations?hl=en&user=UpymOMwAAAAJ&view_op=list_works&sortby=pubdate">Google Scholar</a>]&nbsp;
    [<a href="http://github.com/david-yoon/">GitHub</a>]&nbsp;
    [<a href="http://www.linkedin.com/in/david-s-yoon/">LinkedIn</a>]&nbsp;
    [<a href="http://twitter.com/david_s_yoon/">twitter</a>]
<br>

<br><br>







<hr>
<p>
I am a Research Scientist at Adobe Research. My research interests are in the areas of <strong>machine learning</strong> and <strong>natural language processing (NLP)</strong>. I am particularly interested in understanding long texts for question answering systems and learning language representation for NLP tasks. Further interests lie in applying and integrating NLP research with other disciplines to tackle practical issues; understanding multimodal information (i.e., text, audio, and visual) and NLP for social good.
    
    
<p> I received my Ph.D. in Electrical and Computer Engineering from Seoul National University in 2020 with the Distinguished Dissertation Award, where I was fortunate to be advised by <a href="http://milab.snu.ac.kr/kjung/index.html" title="Kyomin Jung">Dr. Kyomin Jung</a>.    
Prior to Seoul National University, I had involved critical initiatives for the engineering and innovation of AI and machine learning while I was a staff software engineer at Samsung Research Artificial Intelligence Center (2006-2017).


<!--
<p>I have published studies at NLP, AI, or signal processing conferences, such as <strong>ACL</strong>, <strong>NAACL</strong>, <strong>AAAI</strong>, <strong>CIKM</strong>, and <strong>ICASSP</strong>.
</p>

<p>
    My research interests are in the areas of <strong>machine learning</strong> and <strong>natural language processing (NLP)</strong>. I am particularly interested in understanding long texts for question answering systems and learning language representation for NLP tasks. Further interests lie in applying and integrating NLP research with other disciplines to tackle practical issues; understanding multimodal information (i.e., text, audio, and visual) and NLP for social good.

<p>My  Ph.D. advisor is <a href="http://milab.snu.ac.kr/kjung/index.html" title="Kyomin Jung">Dr. Kyomin Jung</a>, Dept. of Electrical and Computer Engineering, Seoul National University. Before coming to SNU, I was a staff software engineer at Samsung Research AI Center.
    
I have published studies at NLP, AI, or signal processing conferences, such as <strong>ACL</strong>, <strong>NAACL</strong>, <strong>AAAI</strong>, <strong>CIKM</strong>, and <strong>ICASSP</strong>.
</p>
-->


<!--<p><strong>Keywords:</strong> NLP, Machine Learning, Artificial Intelligence</p>-->
<hr>

<h3>News</h3>

<ul style="line-height:1.4em">
  <font size="2">
  <li>
	<font color=red>*new*</font>
	[05/2025]
	Our team achieved <b>2nd place</b> in the AVERITEC Shared Task hosted by <a href="https://2025.aclweb.org/">ACL 2025</a> Workshop <a href="https://fever.ai/task.html">FEVER</a>.
  </li>
  <li>
	<font color=red>*new*</font>
	[05/2025]
	<b>Three papers</b> (Active Learning, GUI Agent (findings), IR (findings)) are accepted to <a href="https://2025.aclweb.org/">ACL 2025</a>.
  </li>
  <li>
	<font color=red>*new*</font>
	[05/2025]
	<b>Two papers</b> (Image Captioning, LLM long context) are accepted to <a href="https://icml.cc/">ICML 2025</a>.
  </li>
  <li>
	<font color=red>*new*</font>
	[01/2025]
	<b>Three papers</b> (QA, Inductive Reasoning, LVLM) are accepted to <a href="https://2025.naacl.org/">NAACL 2025</a>.
  </li>
  <li>
	[01/2025]
	<b>Two papers</b> (Text to ReLighting (main) and Domain specific QA (workshop) ) are accepted to <a href="https://2024.emnlp.org/">AAAI 2025</a>.
  </li>
  <li>
    [12/2024]    	
    I gave a talk at <a href="https://en.snu.ac.kr/index.html">SNU</a>, "Capability of Vision and Language Model for "<b>AI for Social Good</b>"
  </li>
  <li>
    [09/2024]    	
    I gave a talk at <a href="http://neweng.cau.ac.kr">CAU</a>, "Vision and Language Representation with LLM for Multimodal Understanding"	
  </li>
  <li>
	[09/2024]
	<b>Three papers</b> (Summarization and Factual Inconsistency Detection, Document QA) are accepted to <a href="https://2024.emnlp.org/">EMNLP 2024</a>.
  </li>
  <li>
	[08/2024]
	Our team achieved <b>2nd place</b> (<b>1st place among open source</b>) in the AVERITEC Shared Task hosted by <a href="https://2024.emnlp.org/">EMNLP 2024</a> Workshop <a href="https://fever.ai/task.html">FEVER</a>.
  </li>
  <li>
	[07/2024]
	One paper (Video Localization) is accepted to <a href="https://cikm2024.org/">CIKM 2024</a>.
  </li>
  <li>
	[06/2024]
	One paper (Speaker Identification) is accepted to <a href="https://interspeech2024.org/">INTERSPEECH 2024</a>.
  </li>
  <li>
	[05/2024]
	One paper (Assessing News Thumbnail Representativeness) is accepted to <a href="https://2024.aclweb.org/">ACL 2024 Findings</a>.
  </li>
  <li>
	[03/2024]
	One paper (Multilingual Representation for Semantic Retrieval) is accepted to <a href="https://sigir-2024.github.io/">SIGIR 2024</a>.
  </li>
  <li>
	[03/2024]
	One paper (Explainable Image Classification) is accepted to <a href="https://2024.naacl.org/">NAACL 2024 Findings</a>.
  </li>
  <li>
	[02/2024]
	One paper (Video Summary) is accepted to <a href="https://cvpr.thecvf.com/">CVPR 2024</a>.
  </li>
  <li>
	[01/2024]
	One paper (Textual Representation) is accepted to <a href="https://2024.eacl.org/">EACL 2024 Findings</a>.
  </li>
</ul>

<a href="{{ site.baseurl }}/news.html">history</a>    
<hr>

<h3>Academic Activities</h3>
<ul style="line-height:1.4em">
  <li>
	  <strong><font color=darkblue>Service: </font></strong> <br>
	  <strong>Program Committee</strong>, NAACL (since 2019), ACL (since 2020), EMNLP (since 2019), AACL (since 2020), EACL (since 2021), COLING (since 2022), LREC (since 2023), ARR (since 2022) <br>
    <strong>Program Committee</strong>, AAAI (since 2020), WWW (since 2021), INTERSPEECH (2019), ICLR (since 2023) <br>
    <strong>Journal Reviewer</strong>, Information Processing and Management, 2020 <br>
    <strong>Journal Reviewer</strong>, IEEE Signal Processing Letters, 2020 <br>
    <br>
  </li>
  
  <li>
  <strong><font color=darkblue>Invited Talks: </font></strong> <br>
    Capability of Vision and Language Model for "AI for Social Good, <strong>Seoul National University.</strong>, Dec. 2024<br>	
    Vision and Language Representation with LLM for Multimodal Understanding, <strong>Chung-Ang University.</strong>, Sep. 2024<br>
    Robust Textual Representation for Text and Multimodal Understanding, <strong>Dongguk University.</strong>, Mar. 2024<br>
    Learning Text Representation for NLP Application, <strong>Seoul National Univ.</strong>, Aug. 2023<br>
    Pretrained Language Model and Semantic Textual Understanding, <strong>SKKU</strong>, Sep. 2022<br>
    Semantic Textual Understanding for Information Retrieval, <a href="http://capp.snu.ac.kr/?p=workshop#program">Seoul National Univ.</a>, Aug. 2022<br>
    Mutimodal Evaluation Metric and Image Captioning Model, <strong>Korea Univ.</strong>, Dec. 2021<br>
    Recent Advancements in NLP for QA, LM, and Evaluation Metric, <strong>Dongguk Univ.</strong>, Sep. 2020<br>
    Understanding Long Texts for Question Answering System Using DNN, <strong>KAIST/IBS</strong>, Jul. 2020<br>
    Question Answering System for Long Text, <strong>Adobe Research</strong> (San Jose, CA, US), Dec. 2019<br>
    Question Answering System and Multimodal Speech Emotion Recognition, <strong>DEEPEST</strong>, Aug. 2019<br>
	Research in Natural Language Processing, 
	<a href="https://www.nvidia.com/ko-kr/ai-conference/"><strong>NVIDIA AI Conference</strong></a>, Jul. 2019<br>
	Question Answering for Short Answer, <strong>Adobe Research</strong> (San Jose, CA, US), Dec. 2018<br>
	QA-pair ranking algorithm and its applications, <strong>NAVER</strong>, Aug. 2018<br>
    Learning to Rank Question-Answer Pairs, <strong>PyTorch KR</strong>, Jun. 2018<br>
	Advancement of the Neural Dialogue Model, 
	<a href="https://www.fastcampus.co.kr/data_camp_lab/"><strong>Fast campus</strong></a>, 
	Jul. 2018 <br>
	<br>
  </li>
  <li><font size="2">
	<strong><font color=darkblue>Teaching Assistant: </font></strong> <br>
	Programming Methodology, Seoul National University, Spring 2018 <br>
	Machine Learning, Seoul National University, Fall 2015 <br>
	Lab. Sentiment Analysis, BigCamp (Big Data Academy), Big Data Institute, 2016-2019 <br>
	<br>
  </li>
  </font>
</ul>


<hr>

<h3>Professional Experiences</h3>
<ul style="line-height:1.4em">
	<li><font size="2">
	<strong><font color=darkblue>NLP Research Scientist: </font></strong>
	Adobe Research (San Jose, CA, US),
	2020-present
  </li>
  <li>
	<strong><font color=darkblue>Staff Engineer: </font></strong>
	Samsung Research (Seoul, KR),
	2006-2017
  </li>
  <li>
	<strong><font color=darkblue>Representative of employees: </font></strong>
	Samsung Electronics (Seoul, KR),
	2012-2014
  </li>
  <li>
	<strong><font color=darkblue>Trainer of Global New Employee Course: </font></strong>
	Samsung Electronics (Seoul, KR),
	Spring 2011 
  </li></font></ul>
</ol>
<hr>
<h3>Publications</h3>
<!--*denotes equal contribution.<br>-->
<ol style="line-height:1.4em" reversed>
  <font size="2">
  <h4><strong>[2025]</strong></h4>
	<li>
		<strong>From Selection to Generation: A Survey of LLM-based Active Learning</strong>
		<a href="https://arxiv.org/pdf/2502.11767">[pdf]</a>
		<br><i>Yu Xia, Subhojyoti Mukherjee, Zhouhang Xie, Junda Wu, Xintong Li, Ryan Aponte, Hanjia Lyu, Joe Barrow, Hongjie Chen, Franck Dernoncourt, Branislav Kveton, Tong Yu, Ruiyi Zhang, Jiuxiang Gu, Nesreen K. Ahmed, Yu Wang, Xiang Chen, Hanieh Deilamsalehy, Sungchul Kim, Zhengmian Hu, Yue Zhao, Nedim Lipka, <u>Seunghyun Yoon</u>, Ting-Hao Kenneth Huang, Zichao Wang, Puneet Mathur, Soumyabrata Pal, Koyel Mukherjee, Zhehao Zhang, Namyong Park, Thien Huu Nguyen, Jiebo Luo, Ryan A. Rossi, Julian McAuley</i>
		<br><a href="https://2025.aclweb.org/">ACL 2025</a>
		<p>
	</li>
	<li>
		<strong>Hypothetical Documents or Knowledge Leakage? Rethinking LLM-based Query Expansion</strong>
		<a href="https://arxiv.org/pdf/2504.14175">[pdf]</a>
		<br><i>Yejun Yoon, Jaeyoon Jung, <u>Seunghyun Yoon</u>, Kunwoo Park</i>
		<br><a href="https://2025.aclweb.org/">ACL 2025 Findings</a>
		<p>
	</li>
	<li>
		<strong>Click, Type, Repeat: A Comprehensive Survey on GUI Agents</strong>
		<a href="https://arxiv.org/pdf/2412.13501">[pdf]</a>
		<br><i>Dang Nguyen, Jian Chen, Yu Wang, Gang Wu, Namyong Park, Zhengmian Hu, Hanjia Lyu, Junda Wu, Ryan Aponte, Yu Xia, Xintong Li, Jing Shi, Hongjie Chen, Viet Dac Lai, Zhouhang Xie, Sungchul Kim, Ruiyi Zhang, Tong Yu, Mehrab Tanjim, Nesreen K. Ahmed, Puneet Mathur, <u>Seunghyun Yoon</u>, Lina Yao, Branislav Kveton, Jihyung Kil, Thien Huu Nguyen, Trung Bui, Tianyi Zhou, Ryan A. Rossi, Franck Dernoncourt</i>
		<br><a href="https://2025.aclweb.org/">ACL 2025 Findings</a>
		<p>
	</li>
	<li>
		<strong>Toward Robust Hyper-Detailed Image Captioning: A Multiagent Approach and Dual Evaluation Metrics for Factuality and Coverage</strong>
		<a href="https://arxiv.org/abs/2412.15484">[pdf]</a>
		<a href="https://github.com/adobe-research/CapMAS">[code]</a>
		<a href="https://huggingface.co/datasets/saehyungl/CapMAS/">[data]</a>
		<br><i>Saehyung Lee, <u>Seunghyun Yoon</u>, Trung Bui, Jing Shi, Sungroh Yoon</i>
		<br><a href="https://icml.cc/">ICML 2025</a>
		<p>
	</li>
	<li>
		<strong>NoLiMa: Long-Context Evaluation Beyond Literal Matching</strong>
		<a href="https://arxiv.org/abs/2502.05167">[pdf]</a>
		<a href="https://github.com/adobe-research/NoLiMa">[code]</a>
		<br><i>Ali Modarressi, Hanieh Deilamsalehy, Franck Dernoncourt, Trung Bui, Ryan A. Rossi, <u>Seunghyun Yoon</u>, Hinrich Schuetze</i>
		<br><a href="https://icml.cc/">ICML 2025</a>
		<p>
	</li>
	<li>
		<strong>Generating Diverse Hypotheses for Inductive Reasoning</strong>
		<a href="https://aclanthology.org/2025.naacl-long.429.pdf">[pdf]</a>
		<br><font color=orange>(oral presentation)</font>
		<br><i>Kang-il Lee, Hyukhun Koh, Dongryeol Lee, <u>Seunghyun Yoon</u>, Minsung Kim, Kyomin Jung</i>
		<br><a href="https://2025.naacl.org/">NAACL 2025 </a>
		<p>
	</li>
	<li>
		<strong>CORG: Generating Answers from Complex, Interrelated Contexts</strong>
		<a href="https://aclanthology.org/2025.naacl-long.428.pdf">[pdf]</a>
		<a href="https://github.com/adobe-research/CORG">[code]</a>
		<br><i>Hyunji Lee, Franck Dernoncourt, Trung Bui, <u>Seunghyun Yoon</u></i>
		<br><a href="https://2025.naacl.org/">NAACL 2025 </a>
		<p>
	</li>
	<li>
		<strong>VLind-Bench: Measuring Language Priors in Large Vision-Language Models</strong>
		<a href="https://aclanthology.org/2025.findings-naacl.231.pdf">[pdf]</a>
		<a href="https://github.com/klee972/VLind-Bench">[code]</a>
		<a href="https://huggingface.co/datasets/klee972/VLind-Bench">[data]</a>
		<br><i>Kang-il Lee, Minbeom Kim, <u>Seunghyun Yoon</u>, Minsung Kim, Dongryeol Lee, Hyukhun Koh, Kyomin Jung</i>
		<br><a href="https://2025.naacl.org/">NAACL 2025 Findings</a>
		<p>
	</li>
	<li>
		<strong>Text2Relight: Creative Portrait Relighting with Text Guidance</strong>
		<a href="https://arxiv.org/pdf/2412.13734">[pdf]</a>
		<br><i>Junuk Cha, Mengwei Ren, Krishna Kumar Singh, He Zhang, Yannick Hold-Geoffroy, <u>Seunghyun Yoon</u>, HyunJoon Jung, Jae Shin Yoon, Seungryul Baek </i>
		<br><a href="https://aaai.org/conference/aaai/aaai-25/">AAAI 2025</a>
		<p>
	</li>
	<li>
		<strong>Domain-specific Question Answering with Hybrid Search</strong>
		<a href="https://arxiv.org/pdf/2412.03736">[pdf]</a>
		<br><i>Dewang Sultania, Zhaoyu Lu, Twisha Naik, Franck Dernoncourt, <u>Seunghyun Yoon</u>, Sanat Sharma, Trung Bui, Ashok Gupta, Tushar Vatsa, Suhas Suresha, Ishita Verma, Vibha Belavadi, Cheng Chen, Michael Friedrich</i>
		<br><a href="nan">AAAI 2025  Workshop Document Understanding and Intelligence</a>
		<p>
	</li>
  <h4><strong>[2024]</strong></h4>
	<li>
		<strong>HerO at AVeriTeC: The Herd of Open Large Language Models for Verifying Real-World Claims</strong>
		<a href="https://aclanthology.org/2024.fever-1.15.pdf">[pdf]</a>
		<a href="https://github.com/ssu-humane/HerO">[code]</a>
		<br><font color=orange>(2nd place / 1st place among open source)</font>
		<br><i>Yejun Yoon, Jaeyoon Jung, <u>Seunghyun Yoon</u>, Kunwoo Park</i>
		<br><a href="https://fever.ai/workshop.html">EMNLP 2024 Workshop FEVER</a>
		<p>
	</li>
	<li>
		<strong>PDFTriage: Question Answering over Long, Structured Documents</strong>
		<a href="https://aclanthology.org/2024.emnlp-industry.13.pdf">[pdf]</a>
		<br><i>Jon Saad-Falcon, Joe Barrow, Alexa Siu, Ani Nenkova, <u>Seunghyun Yoon</u>, Ryan A. Rossi, Franck Dernoncourt</i>
		<br><a href="https://2024.emnlp.org/">EMNLP 2024 Industry</a>
		<p>
	</li>
	<li>
		<strong>FIZZ: Factual Inconsistency Detection by Zoom-in Summary and Zoom-out Document</strong>
		<a href="https://aclanthology.org/2024.emnlp-main.3.pdf">[pdf]</a>
		<br><i>Joonho Yang, <u>Seunghyun Yoon</u>, Byeongjeong Kim, Hwanhee Lee</i>
		<br><a href="https://2024.emnlp.org/">EMNLP 2024</a>
		<p>
	</li>
	<li>
		<strong>Towards Enhancing Coherence in Extractive Summarization: Dataset and Experiments with LLMs</strong>
		<a href="https://aclanthology.org/2024.emnlp-main.1106.pdf">[pdf]</a>
		<br><i>Mihir Parmar, Hanieh Deilamsalehy, Franck Dernoncourt, <u>Seunghyun Yoon</u>, Ryan A. Rossi, Trung Bui</i>
		<br><a href="https://2024.emnlp.org/">EMNLP 2024</a>
		<p>
	</li>
	<li>
		<strong>A New Framework for Evaluating Faithfulness of Video Moment Retrieval against Multiple Distractors</strong>
		<a href="https://dl.acm.org/doi/pdf/10.1145/3627673.3679838">[pdf]</a>
		<a href="https://github.com/yny0506/Massive-Videos-Moment-Retrieval">[code]</a>
		<br><i>Nakyeong Yang, Minsung Kim, <u>Seunghyun Yoon</u>, Joongbo Shin, Kyomin Jung</i>
		<br><a href="https://cikm2024.org/">CIKM 2024</a>
		<p>
	</li>
	<li>
		<strong>Identifying Speakers in Dialogue Transcripts: A Text-based Approach Using Pretrained Language Models</strong>
		<a href="https://www.isca-archive.org/interspeech_2024/nguyen24_interspeech.pdf">[pdf]</a>
		<br><i>Van Minh Nguyen, Franck Dernoncourt, <u>Seunghyun Yoon</u>, Hanieh Deilamsalehy, Hao Tan, Ryan Rossi, Quan Hung Tran, Trung Bui, Thien Nguyen</i>
		<br><a href="https://interspeech2024.org/">INTERSPEECH 2024</a>
		<p>
	</li>
	<li>
		<strong>Assessing News Thumbnail Representativeness: Counterfactual text can enhance the cross-modal matching ability</strong>
		<a href="https://aclanthology.org/2024.findings-acl.534.pdf">[pdf]</a>
		<a href="https://github.com/ssu-humane/news-images-acl24">[code]</a>
		<br><i>Yejun Yoon, <u>Seunghyun Yoon</u>, Kunwoo Park</i>
		<br><a href="https://2024.aclweb.org/">ACL 2024 Findings</a>
		<p>
	</li>
	<li>
		<strong>Multi-hop Database Reasoning with Virtual Knowledge Graph</strong>
		<a href="https://aclanthology.org/2024.kallm-1.1.pdf">[pdf]</a>
		<br><i>Juhee Son, Yeon Seonwoo, Alice Oh, James Thorne, <u>Seunghyun Yoon</u></i>
		<br><a href="https://2024.aclweb.org/">ACL 2024 Workshop KaLLM</a>
		<p>
	</li>
	<li>
		<strong>KaPQA: Knowledge-Augmented Product Question-Answering</strong>
		<a href="https://aclanthology.org/2024.knowledgenlp-1.2.pdf">[pdf]</a>
		<br><i>Swetha Eppalapally, Daksh Dangi, Chaithra Bhat, Ankita Gupta, Ruiyi Zhang, Karishma Bagga, <u>Seunghyun Yoon</u>, Nedim Lipka, Ryan A. Rossi, Franck Dernoncourt</i>
		<br><a href="https://2024.aclweb.org/">ACL 2024 Workshop KnowledgeNLP</a>
		<p>
	</li>
	<li>
		<strong>Multilingual Sentence-Level Semantic Search using Meta-Distillation Learning</strong>
		<a href="https://dl.acm.org/doi/pdf/10.1145/3626772.3657812">[pdf]</a>
		<br><i>Meryem M'hamdi, Jonathan May, Franck Dernoncourt, Trung Bui, <u>Seunghyun Yoon</u></i>
		<br><a href="https://sigir-2024.github.io/">SIGIR 2024</a>
		<p>
	</li>
	<li>
		<strong>PEEB: Part-based Bird Classifiers with an Explainable and Editable Language Bottleneck</strong>
		<a href="https://aclanthology.org/2024.findings-naacl.131.pdf">[pdf]</a>
		<br><i>Thang M. Pham, Peijie Chen, Tin Nguyen, <u>Seunghyun Yoon</u>, Trung Bui, Anh Nguyen</i>
		<br><a href="https://2024.naacl.org/">NAACL 2024 Findings</a>
		<p>
	</li>
	<li>
		<strong>Scaling Up Video Summarization Pretraining with Large Language Models</strong>
		<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Argaw_Scaling_Up_Video_Summarization_Pretraining_with_Large_Language_Models_CVPR_2024_paper.pdf">[pdf]</a>
		<br><i>Dawit Mureja Argaw, <u>Seunghyun Yoon</u>, Fabian Caba Heilbron, Hanieh Deilamsalehy, Trung Bui, Zhaowen Wang, Franck Dernoncourt, Joon Son Chung </i>
		<br><a href="https://cvpr.thecvf.com/">CVPR 2024</a>
		<p>
	</li>
	<li>
		<strong>Fine-tuning CLIP Text Encoders with Two-step Paraphrasing</strong>
		<a href="https://aclanthology.org/2024.findings-eacl.144.pdf">[pdf]</a>
		<br><i>Hyunjae Kim, <u>Seunghyun Yoon</u>, Trung Bui, Handong Zhao, Quan Tran, Franck Dernoncourt, Jaewoo Kang</i>
		<br><a href="https://2024.eacl.org/">EACL 2024 Findings</a>
		<p>
	</li>
	<li>
		<strong>Retrieval Augmented Generation for Domain-specific Question</strong>
		<a href="https://arxiv.org/pdf/2404.14760">[pdf]</a>
		<br><i>Sanat Sharma, <u>Seunghyun Yoon</u>, Franck Dernoncourt, Dewang Sultania, Karishma Bagga, Mengjiao Zhang, Trung Bui, Varun Kotte</i>
		<br><a href="https://sites.google.com/view/sdu-aaai24">AAAI 2024 Workshop SDU</a>
		<p>
	</li>
	<li>
		<strong>Multi-Modal Video Topic Segmentation with Dual-Contrastive Domain Adaptation</strong>
		<a href="https://arxiv.org/pdf/2312.00220.pdf">[pdf]</a>
		<br><i>Linzi Xing, Quan Tran, Fabian Caba Heilbron, Franck Dernoncourt, <u>Seunghyun Yoon</u>, Zhaowen Wang, Trung Bui, Giuseppe Carenini</i>
		<br><a href="https://mmm2024.org/">Multimedia Modelling 2024</a>
		<p>
	</li>
  <h4><strong>[2023]</strong></h4>
	<li>
		<strong>Aspect-based Meeting Transcript Summarization: A Two-Stage Approach with Weak Supervision on Sentence Classification</strong>
		<a href="https://arxiv.org/pdf/2311.04292.pdf">[pdf]</a>
		<br><i>Zhongfen Deng, <u>Seunghyun Yoon</u>, Trung Bui, Franck Dernoncourt, Quan Tran, Shuaiqi Liu, Wenting Zhao, Tao Zhang, Yibo Wang, Philip Yu</i>
		<br><a href="https://bigdataieee.org/BigData2023/">IEEE BigData 2023</a>
		<p>
	</li>
	<li>
		<strong>Perturbation Robust Metric for Multi-Lingual Image Captioning</strong>
		<a href="https://aclanthology.org/2023.findings-emnlp.819.pdf">[pdf]</a>
		<br><i>Yongil Kim, Yerin Hwang, Hyeongu Yun, <u>Seunghyun Yoon</u>, Trung Bui, Kyomin Jung</i>
		<br><a href="https://2023.emnlp.org/">EMNLP 2023 Findings</a>
		<p>
	</li>
	<li>
		<strong>Moment Detection in Long Tutorial Videos</strong>
		<a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Croitoru_Moment_Detection_in_Long_Tutorial_Videos_ICCV_2023_paper.pdf">[pdf]</a>
		<a href="https://github.com/ioanacroi/longmoment-detr">[code]</a>
		<br><i>Ioana Croitoru, Simion-Vlad Bogolin, Samuel Albanie, Yang Liu, Zhaowen Wang, <u>Seunghyun Yoon</u>, Franck Dernoncourt, Hailin Jin, Trung Bui</i>
		<br><a href="https://iccv2023.thecvf.com/">ICCV 2023</a>
		<p>
	</li>
	<li>
		<strong>Boosting Punctuation Restoration with Data Generation and Reinforcement Learning</strong>
		<a href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/lai23c_interspeech.pdf">[pdf]</a>
		<br><i>Viet Lai, Abel Salinas, Hao Tan, Trung Bui, Quan Tran, <u>Seunghyun Yoon</u>, Hanieh Deilamsalehy, Franck Dernoncourt, Thien Nguyen</i>
		<br><a href="https://www.interspeech2023.org/">INTERSPEECH 2023</a>
		<p>
	</li>
	<li>
		<strong>Automatic Creation of Named Entity Recognition Datasets by Querying Phrase Representations</strong>
		<a href="https://aclanthology.org/2023.acl-long.394.pdf">[pdf]</a>
		<br><i>Hhynjae Kim, Jaehyo Yoo, <u>Seunghyun Yoon</u>, Jaewoo Kang</i>
		<br><a href="https://2023.aclweb.org/">ACL 2023</a>
		<p>
	</li>
	<li>
		<strong>MEETINGQA: Extractive Question-Answering on Meeting Transcripts</strong>
		<a href="https://aclanthology.org/2023.acl-long.837.pdf">[pdf]</a>
		<br><i>Archiki Prasad, Trung Bui, <u>Seunghyun Yoon</u>, Hanieh Deilamsalehy, Franck Dernoncourt, Mohit Bansal</i>
		<br><a href="https://2023.aclweb.org/">ACL 2023</a>
		<p>
	</li>
	<li>
		<strong>PiC: A Phrase-in-Context Dataset for Phrase Understanding and Semantic Search</strong>
		<a href="https://aclanthology.org/2023.eacl-main.1.pdf">[pdf]</a>
		<a href="https://phrase-in-context.github.io/">[page]</a>
		<br><i>Thang M. Pham, <u>Seunghyun Yoon</u>, Trung Bu, Anh Nguyeng</i>
		<br><a href="https://2023.eacl.org/">EACL 2023</a>
		<p>
	</li>
  <h4><strong>[2022]</strong></h4>
	<li>
		<strong>Factual Error Correction for Abstractive Summaries Using Entity Retrieval</strong>
		<a href="https://aclanthology.org/2022.gem-1.41.pdf">[pdf]</a>
		<br><i>Hwanhee Lee, Cheoneum Park, <u>Seunghyun Yoon</u>, Trung Bu, Franck Dernoncourt, Juae Kim, Kyomin Jung</i>
		<br><a href="https://2022.emnlp.org/">EMNLP 2022 Workshop GEM</a>
		<p>
	</li>
	<li>
		<strong>Improving cross-modal attention via object detection</strong>
		<a href="https://attention-learning-workshop.github.io/2022/papers/kim-improving_crossmodal_attention_via_object_detection.pdf">[pdf]</a>
		<br><i>Yongil Kim, Yerin Hwang, <u>Seunghyun Yoon</u>, Hyeongu Yun, Kyomin Jung</i>
		<br><a href="https://nips.cc/">NeurIPS 2022 Workshop All Things Attention</a>
		<p>
	</li>
	<li>
		<strong>Simple Questions Generate Named Entity Recognition Datasets</strong>
		<a href="https://aclanthology.org/2022.emnlp-main.417.pdf">[pdf]</a>
		<a href="https://github.com/dmis-lab/GeNER">[code]</a>
		<br><i>Hyunjae Kim, Jaehyo Yoo, <u>Seunghyun Yoon</u>, Jinhyuk Lee, Jaewoo Kang</i>
		<br><a href="https://2022.emnlp.org/">EMNLP 2022</a>
		<p>
	</li>
	<li>
		<strong>Virtual Knowledge Graph Construction for Zero-Shot Domain-Specific Document Retrieval</strong>
		<a href="https://aclanthology.org/2022.coling-1.101.pdf">[pdf]</a>
		<br><i>Yeon Seonwoo, <u>Seunghyun Yoon</u>, Franck Dernoncourt, Trung Bui, Alice Oh</i>
		<br><a href="https://coling2022.org/">COLING 2022</a>
		<p>
	</li>
	<li>
		<strong>Medical Question Understanding and Answering with Knowledge Grounding and Semantic Self-Supervision</strong>
		<a href="https://aclanthology.org/2022.coling-1.241.pdf">[pdf]</a>
		<br><i>Khalil Mrini, Harpreet Singh, Franck Dernoncourt, <u>Seunghyun Yoon</u>, Trung Bui, Walter W. Chang, Emilia Farcas, Ndapa Nakashole</i>
		<br><a href="https://coling2022.org/">COLING 2022</a>
		<p>
	</li>
	<li>
		<strong>Offensive Content Detection Via Synthetic Code-Switched Text</strong>
		<a href="https://aclanthology.org/2022.coling-1.575.pdf">[pdf]</a>
		<br><i>Cesa Salaam, Franck Dernoncourt, Trung Bui, <u>Seunghyun Yoon</u></i>
		<br><a href="https://coling2022.org/">COLING 2022</a>
		<p>
	</li>
	<li>
		<strong>Keyphrase Prediction from Video Transcripts: New Dataset and Directions</strong>
		<a href="https://aclanthology.org/2022.coling-1.624.pdf">[pdf]</a>
		<br><i>Amir Pouran Ben Veyseh, Quan Tran, <u>Seunghyun Yoon</u>, Varun Manjunatha, Hanieh Deilamsalehy, Rajiv Jain, Trung Bui, Walter W. Chang, Franck Dernoncourt, Thien Huu Nguyen</i>
		<br><a href="https://coling2022.org/">COLING 2022</a>
		<p>
	</li>
	<li>
		<strong>MACRONYM: A Large-Scale Dataset for Multilingual and Multi-Domain Acronym Extraction </strong>
		<a href="https://aclanthology.org/2022.coling-1.292.pdf">[pdf]</a>
		<br><i>Amir Pouran Ben Veyseh, Nicole Meister, <u>Seunghyun Yoon</u>, Rajiv Jain, Franck Dernoncourt, Thien Huu Nguyen</i>
		<br><a href="https://coling2022.org/">COLING 2022</a>
		<p>
	</li>
	<li>
		<strong>Fine-grained Image Captioning with CLIP Reward</strong>
		<a href="https://aclanthology.org/2022.findings-naacl.39.pdf">[pdf]</a>
		<a href="https://github.com/j-min/CLIP-Caption-Reward">[code]</a>
		<a href="https://huggingface.co/spaces/NAACL2022/CLIP-Caption-Reward">[demo]</a>
		<br><i>Jaemin Cho, <u>Seunghyun Yoon</u>, Ajinkya Kale, Franck Dernoncourt, Trung Bui, M Bansal</i>
		<br><a href="https://2022.naacl.org/">NAACL 2022 Findings</a>
		<p>
	</li>
	<li>
		<strong>Multimodal Intent Discovery from Livestream Videos</strong>
		<a href="https://aclanthology.org/2022.findings-naacl.36.pdf">[pdf]</a>
		<a href="https://github.com/adymaharana/VideoIntentDiscovery">[code]</a>
		<br><i>Adyasha Maharana, Quan Tran, <u>Seunghyun Yoon</u>, Franck Dernoncourt, Trung Bui, Walter Chang, M Bansal</i>
		<br><a href="https://2022.naacl.org/">NAACL Findings 2022</a>
		<p>
	</li>
	<li>
		<strong>How does fake news use a thumbnail? CLIP-based Multimodal Detection on the Unrepresentative News Image</strong>
		<a href="https://aclanthology.org/2022.constraint-1.10.pdf">[pdf]</a>
		<a href="https://github.com/ssu-humane/fake-news-thumbnail?tab=readme-ov-file">[code]</a>
		<br><i>Hyewon Choi, Yejun Yoon, <u>Seunghyun Yoon</u>, Kunwoo Park</i>
		<br><a href="https://lcs2.iiitd.edu.in/CONSTRAINT-2022/">ACL CONSTRAINT 2022</a>
		<p>
	</li>
	<li>
		<strong>CAISE: Conversational Agent for Image Search and Editing</strong>
		<a href="https://cdn.aaai.org/ojs/21337/21337-13-25350-1-2-20220628.pdf">[pdf]</a>
		<a href="https://github.com/hyounghk/CAISE">[code]</a>
		<br><i>Hyounghun Kim, Doo Soon Kim, <u>Seunghyun Yoon</u>, Franck Dernoncourt, Trung Bui, Mohit Bansal</i>
		<br><a href="https://aaai.org/Conferences/AAAI-22/">AAAI 2022</a>
		<p>
	</li>
  <h4><strong>[2021]</strong></h4>
	<li>
		<strong>Few-Shot Intent Detection via Contrastive Pre-Training and Fine-Tuning</strong>
		<a href="https://aclanthology.org/2021.emnlp-main.144.pdf">[pdf]</a>
		<br><i>J Zhang, T Bui, <u>S Yoon</u>, X Chen, Z Liu, C Xia, QH Tran, W Chang, P Yue</i>
		<br><a href="https://2021.emnlp.org/">EMNLP 2021</a>
		<p>
	</li>
	<li>
		<strong>QACE: Asking Questions to Evaluate an Image Caption</strong>
		<a href="https://aclanthology.org/2021.findings-emnlp.395.pdf">[pdf]</a>
		<a href="https://github.com/hwanheelee1993/QACE">[code]</a>
		<br><i>H Lee, T Scialom, <u>S Yoon</u>, F Dernoncourt, K Jung</i>
		<br><a href="https://2021.emnlp.org/">EMNLP 2021 Findings</a>
		<p>
	</li>
	<li>
		<strong>A Gradually Soft Multi-Task and Data-Augmented Approach to Medical Question Understanding</strong>
		<a href="https://aclanthology.org/2021.acl-long.119.pdf">[pdf]</a>
		<br><i>K Mrini, F Dernoncourt, <u>S Yoon</u>, T Bui, W Chang, E Farcas, N Nakashole</i>
		<br><a href="https://2021.aclweb.org/">ACL 2021</a>
		<p>
	</li>
	<li>
		<strong>UMIC: An Unreferenced Metric for Image Captioning via Contrastive Learning</strong>
		<a href="https://aclanthology.org/2021.acl-short.29.pdf">[pdf]</a>
		<a href="https://github.com/hwanheelee1993/UMIC">[code]</a>
		<br><i>H Lee, <u>S Yoon</u>, F Dernoncourt, T Bui, K Jung</i>
		<br><a href="https://2021.aclweb.org/">ACL 2021</a>
		<p>
	</li>
	<li>
		<strong>UCSD-Adobe at MEDIQA 2021: Transfer Learning and Answer Sentence Selection for Medical Summarization</strong>
		<a href="https://www.aclweb.org/anthology/2021.bionlp-1.28.pdf">[pdf]</a>
		<br><i>K Mrini, F Dernoncourt, <u>S Yoon</u>, T Bui, W Chang, E Farcas, N Nakashole</i>
		<br><a href="https://aclweb.org/aclwiki/BioNLP_Workshop">NAACL 2021 Workshop BioNLP</a>
		<p>
	</li>
	<li>
		<strong>KPQA: A Metric for Generative Question Answering Using Keyphrase Weights</strong>
		<a href="https://www.aclweb.org/anthology/2021.naacl-main.170.pdf">[pdf]</a>
		<a href="https://github.com/hwanheelee1993/KPQA">[code]</a>
		<br><i>H Lee, <u>S Yoon</u>, F Dernoncourt, DS Kim, T Bui, J Shin, K Jung</i>
		<br><a href="https://2021.naacl.org/">NAACL 2021</a>
		<p>
	</li>
	<li>
		<strong>Learning to Detect Incongruence in News Headline and Body Text via a Graph Neural Network</strong>
		<a href="https://ieeexplore.ieee.org/document/9363185">[pdf]</a>
		<a href="https://github.com/minwhoo/detecting-incongruity-gnn">[code]</a>
		<br><font color=orange>(SCI, IF=3.745)</font>
		<br><i><u>S Yoon</u>*, K Park*, M Lee, T Kim, M Cha, K Jung</i>
		<br><a href="https://ieeeaccess.ieee.org/">IEEE Access 2021</a>
		<p>
	</li>
  <h4><strong>[2020]</strong></h4>
	<li>
		<strong>Collaborative Training of GANs in Continuous and Discrete Spaces for Text Generation</strong>
		<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9296209">[pdf]</a>
		<br><font color=orange>(SCI, IF=3.745)</font>
		<br><i>Y Kim, S Won, <u>S Yoon</u>, K Jung</i>
		<br><a href="https://ieeeaccess.ieee.org/">IEEE Access 2020</a>
		<p>
	</li>
	<li>
		<strong>ViLBERTScore: Evaluating Image Caption Using Vision-and-Language BERT</strong>
		<a href="https://www.aclweb.org/anthology/2020.eval4nlp-1.4.pdf">[pdf]</a>
		<a href="https://github.com/hwanheelee1993/ViLBERTScore">[code]</a>
		<br><i>H Lee, <u>S Yoon</u>, F Dernoncourt, DS Kim, T Bui, K Jung</i>
		<br><a href="https://nlpevaluation2020.github.io/index.html">EMNLP 2020 Workshop Eval4NLP</a>
		<p>
	</li>
	<li>
		<strong>Multimodal Speech Emotion Recognition using Cross Attention with Aligned Audio and Text</strong>
		<a href="https://www.isca-archive.org/interspeech_2020/lee20e_interspeech.pdf">[pdf]</a>
		<br><i>Y Lee, <u>S Yoon</u>, K Jung</i>
		<br><a href="https://arxiv.org/abs/2207.12895/">INTERSPEECH 2020</a>
		<p>
	</li>
	<li>
		<strong>Fast and Accurate Deep Bidirectional Language Representations for Unsupervised Learning</strong>
		<a href="https://www.aclweb.org/anthology/2020.acl-main.76.pdf">[pdf]</a>
		<br><i>J Shin, Y Lee, <u>S Yoon</u>, K Jung</i>
		<br><a href="https://acl2020.org/">ACL 2020</a>
		<p>
	</li>
	<li>
		<strong>Propagate-Selector: Detecting Supporting Sentences for Question Answering via Graph Neural Networks</strong>
		<a href="https://www.aclweb.org/anthology/2020.lrec-1.664">[pdf]</a>
		<a href="https://github.com/david-yoon/propagate-selector">[code]</a>
		<br><i><u>S Yoon</u>, F Dernoncourt, DS Kim, T Bui, K Jung</i>
		<br><a href="https://lrec2020.lrec-conf.org/">LREC 2020</a>
		<p>
	</li>
	<li>
		<strong>Drug-disease Graph: Predicting Adverse Drug Reaction Signals via Graph Neural Network with Clinical Data</strong>
		<a href="https://arxiv.org/pdf/2004.00407.pdf">[pdf]</a>
		<a href="https://pakdd2020.org/download/conference_paper_slides/main-851.pdf">[slide]</a>
		<br><font color=orange>(oral presentation)</font>
		<br><i>H Kwak, M Lee, <u>S Yoon</u>, J Chang, S Park, K Jung</i>
		<br><a href="https://pakdd2020.org/">PAKDD 2020</a>
		<p>
	</li>
	<li>
		<strong>DSTC8-AVSD: Multimodal Semantic Transformer Network with Retrieval Style Word Generator</strong>
		<a href="https://arxiv.org/pdf/2004.08299.pdf">[pdf]</a>
		<br><i>H Lee, <u>S Yoon</u>, F Dernoncourt, DS Kim, T Bui, K Jung</i>
		<br><a href="https://aaai.org/Conferences/AAAI-20/ws20workshops/#ws09">AAAI 2020  Workshop DSTC8</a>
		<p>
	</li>
	<li>
		<strong>Comparative Studies on Machine Learning for Paralinguistic Signal Compression and Classification</strong>
		<a href="https://link.springer.com/content/pdf/10.1007/s11227-020-03346-3.pdf">[pdf]</a>
		<br><font color=orange>(SCI, IF=2.157)</font>
		<br><i>S Byun*, <u>S Yoon</u>*, K Jung</i>
		<br><a href="https://www.springer.com/journal/11227/?gclid=EAIaIQobChMIrJqOzti96QIVBhdgCh1UVAn_EAAYASAAEgLwp_D_BwE/">Journal of Supercomputing 2020</a>
		<p>
	</li>
	<li>
		<strong>Attentive Modality Hopping Mechanism for Speech Emotion Recognition</strong>
		<a href="https://david-yoon.github.io/assets/paper/yoon2020attentive.pdf">[pdf]</a>
		<a href="https://github.com/david-yoon/attentive-modality-hopping-for-SER">[code]</a>
		<a href="https://www.slideshare.net/DavidSeunghyunYoon/slide-attentive-modality-hopping-mechanism-for-speech-emotion-recognition">[slide]</a>
		<br><font color=orange>(oral presentation)</font>
		<br><i><u>S Yoon</u>, S Dey, H Lee, K Jung</i>
		<br><a href="https://2020.ieeeicassp.org/">IEEE ICASSP 2020</a>
		<p>
	</li>
	<li>
		<strong>BaitWatcher: A lightweight web interface for the detection of incongruent news headlines</strong>
		<a href="https://arxiv.org/pdf/2003.11459.pdf">[pdf]</a>
		<a href="https://www.springer.com/gp/book/9783030426989">[book]</a>
		<br><i>K Park, T Kim, <u>S Yoon</u>, M Cha, K Jung</i>
		<br><a href="https://www.springer.com/gp/book/9783030426989">Disinformation, Misinformation, and Fake News in Social Media-Emerging Research Challenges and Opportunities, Springer 2020</a>
		<p>
	</li>
  <h4><strong>[2019]</strong></h4>
	<li>
		<strong>A Compare-Aggregate Model with Latent Clustering for Answer Selection</strong>
		<a href="https://arxiv.org/pdf/1905.12897.pdf">[pdf]</a>
		<a href="https://www.slideshare.net/DavidSeunghyunYoon/slide-a-compareaggregate-model-with-latent-clustering-for-answer-selection">[slide]</a>
		<a href="https://www.slideshare.net/DavidSeunghyunYoon/poster-a-compareaggregate-model-with-latent-clustering-for-answer-selection">[poster]</a>
		<br><font color=orange>(oral presentation)</font>
		<br><i><u>S Yoon</u>, F Dernoncourt, DS Kim, T Bui, K Jung</i>
		<br><a href="http://www.cikm2019.net/">CIKM 2019</a>
		<p>
	</li>
	<li>
		<strong>Surf at MEDIQA 2019: Improving Performance of Natural Language Inference in the Clinical Domain by Adopting Pre-trained Language Model</strong>
		<a href="https://www.aclweb.org/anthology/W19-5043.pdf">[pdf]</a>
		<br><i>J Nam, <u>S Yoon</u>, K Jung</i>
		<br><a href="https://aclweb.org/aclwiki/BioNLP_Workshop">ACL 2019 Workshop BioNLP</a>
		<p>
	</li>
	<li>
		<strong>Speech Emotion Recognition Using Multi-hop Attention Mechanism</strong>
		<a href="https://arxiv.org/pdf/1904.10788.pdf">[pdf]</a>
		<a href="https://sigport.org/documents/speech-emotion-recognition-using-multi-hop-attention-mechanism">[slide]</a>
		<br><font color=orange>(oral presentation)</font>
		<br><i><u>S Yoon</u>, S Byun, S Dey, K Jung</i>
		<br><a href="https://2019.ieeeicassp.org/">IEEE ICASSP 2019</a>
		<p>
	</li>
	<li>
		<strong>Neural Networks for Compressing and Classifying Speaker-Independent Paralinguistic Signals</strong>
		<a href="http://milab.snu.ac.kr/pub/BigComp2019Byun.pdf">[pdf]</a>
		<br><i>S Byun, <u>S Yoon</u>, K Jung</i>
		<br><a href="http://www.bigcomputing.org/">IEEE BigComp 2019</a>
		<p>
	</li>
	<li>
		<strong>Detecting Incongruity Between News Headline and Body Text via a Deep Hierarchical Encoder </strong>
		<a href="https://wvvw.aaai.org/ojs/index.php/AAAI/article/view/3756/3634">[pdf]</a>
		<a href="https://github.com/david-yoon/detecting-incongruity">[code]</a>
		<br><font color=orange>(oral presentation)</font>
		<br><i><u>S Yoon</u>*, K Park*, J Shin, H Lim, S Won, M Cha, K Jung</i>
		<br><a href="https://aaai.org/Conferences/AAAI-19/">AAAI 2019</a>
		<p>
	</li>
  <h4><strong>[2018 and earlier]</strong></h4>
	<li>
		<strong>Multimodal Speech Emotion Recognition using Audio and Text </strong>
		<a href="https://arxiv.org/pdf/1810.04635.pdf">[pdf]</a>
		<a href="https://github.com/david-yoon/multimodal-speech-emotion">[code]</a>
		<br><i><u>S Yoon</u>, S Byun, K Jung</i>
		<br><a href="http://www.slt2018.org/">IEEE SLT 2018</a>
		<p>
	</li>
	<li>
		<strong>Comparative Studies of Detecting Abusive Language on Twitter</strong>
		<a href="https://www.aclweb.org/anthology/W18-5113.pdf">[pdf]</a>
		<a href="https://github.com/younggns/comparative-abusive-lang">[code]</a>
		<br><i>Y Lee*, <u>S Yoon</u>*, K Jung</i>
		<br><a href="https://sites.google.com/view/alw2018">EMNLP ALW 2018</a>
		<p>
	</li>
	<li>
		<strong>Learning to Rank Question-Answer Pairs using Hierarchical Recurrent Encoder with Latent Topic Clustering</strong>
		<a href="https://www.aclweb.org/anthology/N18-1142.pdf">[pdf]</a>
		<a href="https://github.com/david-yoon/QA_HRDE_LTC">[code]</a>
		<br><i><u>S Yoon</u>, J Shin, K Jung</i>
		<br><a href="http://naacl2018.org/">NAACL 2018</a>
		<p>
	</li>
	<li>
		<strong>Contextual-CNN: A Novel Architecture Capturing Unified Meaning for Sentence Classification</strong>
		<a href="http://milab.snu.ac.kr/pub/BigComp2018.pdf">[pdf]</a>
		<br><i>J Shin, Y Kim, <u>S Yoon</u>, K Jung</i>
		<br><a href="http://www.bigcomputing.org/">IEEE BigComp 2018</a>
		<p>
	</li>
	<li>
		<strong>Synonym Discovery with Etymology-based Word Embeddings</strong>
		<a href="https://arxiv.org/pdf/1709.10445.pdf">[pdf]</a>
		<br><i><u>S Yoon</u>, P Estrada, K Jung</i>
		<br><a href="http://www.ele.uri.edu/ieee-ssci2017/">IEEE SSCI 2017</a>
		<p>
	</li>
	<li>
		<strong>Efficient Transfer Learning Schemes for Personalized Language Modeling using Recurrent Neural Network</strong>
		<a href="http://milab.snu.ac.kr/pub/AAAI2017Yoon.pdf">[pdf]</a>
		<br><i><u>S Yoon</u>, H Yun, Y Kim, G Park, K Jung</i>
		<br><a href="http://crowdai.azurewebsites.net/">AAAI 2017 (Workshop)</a>
		<p>
	</li>
	<li>
		<strong>Automatic Question Answering System for Consumer Product</strong>
		<a href="http://milab.snu.ac.kr/pub/IntelliSys2016Yoon.pdf">[pdf]</a>
		<br><i><u>S Yoon</u>, M Sundar, A Gupta, K Jung</i>
		<br><a href="http://saiconference.com/Conferences/IntelliSys2016%22">IntelliSys 2016</a>
		<p>
	</li>
	<li>
		<strong>Mining the Minds of Customers from Online Chat Logs</strong>
		<a href="https://arxiv.org/abs/1510.01801">[pdf]</a>
		<br><i>K Park, J Kim, J Park, M Cha, J Nam, <u>S Yoon</u>, E Rhim</i>
		<br><a href="http://www.cikm-2015.org/">CIKM 2015</a>
		<p>
	</li>
	<li>
		<strong>Domain Question Answering System</strong>
		<a href="nan">[pdf]</a>
		<br><i><u>S Yoon</u>, E Rhim, D Kim</i>
		<br><a href="nan">KIISE Transactions on Computing Practices 2015</a>
		<p>
	</li>
	<li>
		<strong>Media clips: Implementation of an intuitive media linker</strong>
		<a href="https://david-yoon.github.io/assets/paper/MediaClip_BMSB2011.pdf">[pdf]</a>
		<br><i><u>S Yoon</u>, K Lee, H Shin</i>
		<br><a href="https://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp?punumber=5945017">IEEE BMSB 2011</a>
		<p>
	</li>
  </font>
</ol>
<hr>
<h3>Patents</h3>
<font color=lightblue>
  <h4>[  International Patents ]</h4>
</font>
<ol reversed style="line-height:1.4em"><font size="2">	<li>
		[<font color=magenta>issued</font>]		<strong>SYSTEMS AND METHODS FOR IMAGE PROCESSING USING NATURAL LANGUAGE</strong>
		<br><i>H Kim, DS Kim, <u>S Yoon</u>, F Dernoncourt, T Bui</i>
		<br>US 12,293,577, 6-May-25
		<p>
	</li>
	<li>
		[<font color=magenta>issued</font>]		<strong>GENERATING SYNTHETIC CODE-SWITCHED DATA FOR TRAINING LANGUAGE MODELS</strong>
		<a href="https://patents.google.com/patent/US12242820B2/en?oq=US+12%2c242%2c820">[link]</a>
		<br><i><u>S Yoon</u>, T Bui, F Dernoncourt, C Salaam</i>
		<br>US 12,242,820, 4-Mar-25
		<p>
	</li>
	<li>
		[<font color=magenta>issued</font>]		<strong>BI-DIRECTIONAL RECURRENT ENCODERS WITH MULTI-HOP ATTENTION FOR SPEECH EMOTION RECOGNITION</strong>
		<a href="https://patents.google.com/patent/US12236975B2/en?oq=US+12%2c236%2c975">[link]</a>
		<br><i><u>S Yoon</u>, T Bui, S Dey</i>
		<br>US 12,236,975, 25-Feb-25
		<p>
	</li>
	<li>
		[<font color=magenta>issued</font>]		<strong>IMAGE CAPTIONING</strong>
		<a href="https://patents.google.com/patent/US12210825B2/en?oq=US+12%2c210%2c825">[link]</a>
		<br><i>J Cho, <u>S Yoon</u>, A Kale, F Dernoncourt, T Bui</i>
		<br>US 12,210,825, 28-Jan-25
		<p>
	</li>
	<li>
		[<font color=magenta>issued</font>]		<strong>INTENT DETECTION</strong>
		<a href="https://patents.google.com/patent/US12182524B2/en?oq=US12182524B2">[link]</a>
		<br><i>X Chen, W Chang, T Bui, <u>S Yoon</u>, Q Tran, J Zhang</i>
		<br>US 12,182,524, 31-Dec-24
		<p>
	</li>
	<li>
		[<font color=magenta>issued</font>]		<strong>Multimodal intent discovery system</strong>
		<a href="https://patents.google.com/patent/US12124508B2/en?oq=12124508">[link]</a>
		<br><i>A Maharana, Q Hung Tran, <u>S Yoon</u>, F Dernoncourt, T Huu Bui, W  Chang</i>
		<br>US 12,124,508, 22-Oct-24
		<p>
	</li>
	<li>
		[<font color=magenta>issued</font>]		<strong>Using neural networks to detect incongruence between headlines and body text of documents</strong>
		<a href="https://patents.google.com/patent/US12038960B2/en?oq=12038960">[link]</a>
		<br><i><u>S Yoon</u></i>
		<br>US 12,038,960, 16-Jul-24
		<p>
	</li>
	<li>
		[<font color=magenta>issued</font>]		<strong>Utilizing a graph neural network to identify supporting text phrases and generate digital query responses </strong>
		<a href="https://patents.google.com/patent/US20210058345A1/en?oq=US20210058345A1">[link]</a>
		<br><i><u>S Yoon</u>, F Dernoncourt, DS Kim, T Bui</i>
		<br>US 11,271,876, 8-Mar-22
		<p>
	</li>
	<li>
		[<font color=magenta>issued</font>]		<strong>Utilizing bi-directional recurrent encoders with multi-hop attention for speech emotion recognition</strong>
		<a href="https://patents.google.com/patent/US20210050033A1/en?oq=US20210050033A1">[link]</a>
		<br><i>T Bui, S Dey, <u>S Yoon</u></i>
		<br>US 11,205,444, 21-Dec-21
		<p>
	</li>
	<li>
		[<font color=magenta>issued</font>]		<strong>Answer selection using a compare-aggregate model with language model and condensed similarity information from latent clustering</strong>
		<a href="https://patents.google.com/patent/US11113323B2/en?oq=US11113323B2">[link]</a>
		<br><i><u>S Yoon</u>, F Dernoncourt, T Bui, DS Kim, CI Dockhorn, Y Gong</i>
		<br>US 11,113,323, 7-Sep-21
		<p>
	</li>
	<li>
		[<font color=magenta>issued</font>]		<strong>Terminal apparatus, server and method of controlling the same</strong>
		<a href="https://patents.google.com/patent/US10084850B2/en?oq=US+10%2c084%2c850/">[link]</a>
		<br><i>Y Kim, O Kwon, S Kim, H Oh, <u>S Yoon</u>, S Cha, J Lee</i>
		<br>US 10,084,850, CN 201410085759, EP20140154718, 25-Sep-18
		<p>
	</li>
	<li>
		<strong>Method and device for analyzing user's emotion</strong>
		<a href="https://patentscope.wipo.int/search/en/detail.jsf?docId=WO2016182393">[link]</a>
		<br><i>E Rhim, J Kim, J Nam, <u>S Yoon</u>, K Park, J Park, M Cha</i>
		<br>WO2016182393, 13-May-16
		<p>
	</li>
	<li>
		[<font color=magenta>issued</font>]		<strong>Method of recommending application, mobile terminal using the method, and communication system using the method </strong>
		<a href="https://patents.google.com/patent/US9247376B2/en?oq=US+9%2c247%2c376">[link]</a>
		<br><i>J Nam, M Lee, M Koo, <u>S Yoon</u></i>
		<br>US 9,247,376, 26-Jan-16
		<p>
	</li>
	<li>
		[<font color=magenta>issued</font>]		<strong>Method and apparatus for displaying photo on screen having any shape</strong>
		<a href="https://patents.google.com/patent/US9049383B2/en?oq=US+9%2c049%2c383">[link]</a>
		<br><i><u>S Yoon</u>, M Lee</i>
		<br>US 9,049,383, 2-Jun-15
		<p>
	</li>
	<li>
		[<font color=magenta>issued</font>]		<strong>Method and apparatus for providing information and computer readable storage medium having a program recorded thereon for executing the method </strong>
		<a href="https://patents.google.com/patent/US8958824B2/en?oq=US+8%2c958%2c824">[link]</a>
		<br><i><u>S Yoon</u>, M Lee, M Koo, J Nam</i>
		<br>US 8,958,824, 17-Feb-15
		<p>
	</li>
	<li>
		[<font color=magenta>issued</font>]		<strong>Apparatus and method for clipping and sharing content at a portable terminal</strong>
		<a href="https://patents.google.com/patent/CN103827913A/en?oq=CN103827913A">[link]</a>
		<br><i><u>S Yoon</u>, M Lee, M Koo, J Nam</i>
		<br>US 13/629,394, CN103827913A, EP20120837007, PCT/KR1020110097578, 28-May-14
		<p>
	</li>
	<li>
		[<font color=magenta>issued</font>]		<strong>Method and apparatus for fast tracking position by using global positioning system</strong>
		<a href="https://patents.google.com/patent/US8094070B2/en?oq=US8094070B2">[link]</a>
		<br><i><u>S Yoon</u>, S Kim</i>
		<br>US 8,094,070, 10-Jan-12
		<p>
	</li>
  </font>
</ol>
<font color=lightblue>
  <h4>[ Korean Patents ]</h4>
</font>
<ol reversed style="line-height:1.4em"><font size="2">	<li>
		[<font color=magenta>issued</font>]		<strong>Apparatus and method for evaluating sentense by using bidirectional language model</strong>
		<br><i>K Jung, J Shin, <u>S Yoon</u></i>
		<br>KR 10-2436900, 23-Aug-22
		<p>
	</li>
	<li>
		[<font color=magenta>issued</font>]		<strong>Method and apparatus for emotion recognition based on cross attentionmodel</strong>
		<br><i>K Jung, Y Lee, <u>S Yoon</u></i>
		<br>KR 10-2365433, 16-Feb-22
		<p>
	</li>
	<li>
		[<font color=magenta>issued</font>]		<strong>Artificial intelligence based dialog system and response control method thereof</strong>
		<br><i>K Jung, <u>S Yoon</u>, J Shin, H Kwak, S Byun</i>
		<br>KR 10-2059015, 18-Dec-19
		<p>
	</li>
	<li>
		[<font color=magenta>issued</font>]		<strong>Terminal apparatus, server and method of controlling the same</strong>
		<br><i>Y Kim, O Kwon, S Kim, H Oh, <u>S Yoon</u>, S Cha, J Lee</i>
		<br>KR 10-1832394, 20-Feb-18
		<p>
	</li>
	<li>
		[<font color=magenta>issued</font>]		<strong>Apparatus and method for collecting information of destination in portable terminal</strong>
		<br><i><u>S Yoon</u>, J Nam, M Koo, M Lee</i>
		<br>KR 10-1914632, 29-Oct-18
		<p>
	</li>
	<li>
		[<font color=magenta>issued</font>]		<strong>Method and apparatus for providing information, and computer readable storage medium</strong>
		<br><i><u>S Yoon</u>, M Lee, M Koo, J Nam</i>
		<br>KR 10-1773167, 24-Aug-17
		<p>
	</li>
	<li>
		[<font color=magenta>issued</font>]		<strong>Method for recommendation of application, mobile terminal thereof and communication system thereof</strong>
		<br><i>J Nam, M Lee, M Koo, <u>S Yoon</u></i>
		<br>KR 10-1747303, 8-Jun-17
		<p>
	</li>
	<li>
		<strong>Device and method for analyzing user emotion</strong>
		<br><i>E Rhim, J Kim, J Nam, <u>S Yoon</u>, K Park, J Park, M Cha</i>
		<br>KR 1020160058782, 13-May-16
		<p>
	</li>
	<li>
		[<font color=magenta>issued</font>]		<strong>Method and apparatus for fast positioning using global positioning system</strong>
		<br><i><u>S Yoon</u>, S Kim</i>
		<br>KR 10-1564938, 27-Oct-15
		<p>
	</li>
  </font>
</ol>
