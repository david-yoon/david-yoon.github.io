<hr>
<h3>Publications</h3>
<!--*denotes equal contribution.<br>-->
<ol style="line-height:1.4em" reversed>
  <font size="2">
  <h4><strong>[2025]</strong></h4>
	<li>
		<strong>Toward Robust Hyper-Detailed Image Captioning: A Multiagent Approach and Dual Evaluation Metrics for Factuality and Coverage</strong>
		<a href="https://arxiv.org/abs/2412.15484">[pdf]</a>
		<br><i>Saehyung Lee, <u>Seunghyun Yoon</u>, Trung Bui, Jing Shi, Sungroh Yoon</i>
		<br><a href="https://icml.cc/">ICML 2025</a>
		<p>
	</li>
	<li>
		<strong>NoLiMa: Long-Context Evaluation Beyond Literal Matching</strong>
		<a href="https://arxiv.org/abs/2502.05167">[pdf]</a>
		<br><i>Ali Modarressi, Hanieh Deilamsalehy, Franck Dernoncourt, Trung Bui, Ryan A. Rossi, <u>Seunghyun Yoon</u>, Hinrich Schuetze</i>
		<br><a href="https://icml.cc/">ICML 2025</a>
		<p>
	</li>
	<li>
		<strong>Generating Diverse Hypotheses for Inductive Reasoning</strong>
		<a href="https://aclanthology.org/2025.naacl-long.429.pdf">[pdf]</a>
		<br><font color=orange>(oral presentation)</font>
		<br><i>Kang-il Lee, Hyukhun Koh, Dongryeol Lee, <u>Seunghyun Yoon</u>, Minsung Kim, Kyomin Jung</i>
		<br><a href="https://2025.naacl.org/">NAACL 2025 </a>
		<p>
	</li>
	<li>
		<strong>CORG: Generating Answers from Complex, Interrelated Contexts</strong>
		<a href="https://aclanthology.org/2025.naacl-long.428.pdf">[pdf]</a>
		<br><i>Hyunji Lee, Franck Dernoncourt, Trung Bui, <u>Seunghyun Yoon</u></i>
		<br><a href="https://2025.naacl.org/">NAACL 2025 </a>
		<p>
	</li>
	<li>
		<strong>VLind-Bench: Measuring Language Priors in Large Vision-Language Models</strong>
		<a href="https://aclanthology.org/2025.findings-naacl.231.pdf">[pdf]</a>
		<a href="https://github.com/klee972/VLind-Bench">[code]</a>
		<a href="https://huggingface.co/datasets/klee972/VLind-Bench">[data]</a>
		<br><i>Kang-il Lee, Minbeom Kim, <u>Seunghyun Yoon</u>, Minsung Kim, Dongryeol Lee, Hyukhun Koh, Kyomin Jung</i>
		<br><a href="https://2025.naacl.org/">NAACL 2025 Findings</a>
		<p>
	</li>
	<li>
		<strong>Text2Relight: Creative Portrait Relighting with Text Guidance</strong>
		<a href="https://arxiv.org/pdf/2412.13734">[pdf]</a>
		<br><i>Junuk Cha, Mengwei Ren, Krishna Kumar Singh, He Zhang, Yannick Hold-Geoffroy, <u>Seunghyun Yoon</u>, HyunJoon Jung, Jae Shin Yoon, Seungryul Baek </i>
		<br><a href="https://aaai.org/conference/aaai/aaai-25/">AAAI 2025</a>
		<p>
	</li>
	<li>
		<strong>Domain-specific Question Answering with Hybrid Search</strong>
		<a href="https://arxiv.org/pdf/2412.03736">[pdf]</a>
		<br><i>Dewang Sultania, Zhaoyu Lu, Twisha Naik, Franck Dernoncourt, <u>Seunghyun Yoon</u>, Sanat Sharma, Trung Bui, Ashok Gupta, Tushar Vatsa, Suhas Suresha, Ishita Verma, Vibha Belavadi, Cheng Chen, Michael Friedrich</i>
		<br><a href="nan">AAAI 2025  Workshop Document Understanding and Intelligence</a>
		<p>
	</li>
  <h4><strong>[2024]</strong></h4>
	<li>
		<strong>HerO at AVeriTeC: The Herd of Open Large Language Models for Verifying Real-World Claims</strong>
		<a href="https://aclanthology.org/2024.fever-1.15.pdf">[pdf]</a>
		<a href="https://github.com/ssu-humane/HerO">[code]</a>
		<br><font color=orange>(2nd place / 1st place among open source)</font>
		<br><i>Yejun Yoon, Jaeyoon Jung, <u>Seunghyun Yoon</u>, Kunwoo Park</i>
		<br><a href="https://fever.ai/workshop.html">EMNLP 2024 Workshop FEVER</a>
		<p>
	</li>
	<li>
		<strong>PDFTriage: Question Answering over Long, Structured Documents</strong>
		<a href="https://aclanthology.org/2024.emnlp-industry.13.pdf">[pdf]</a>
		<br><i>Jon Saad-Falcon, Joe Barrow, Alexa Siu, Ani Nenkova, <u>Seunghyun Yoon</u>, Ryan A. Rossi, Franck Dernoncourt</i>
		<br><a href="https://2024.emnlp.org/">EMNLP 2024 Industry</a>
		<p>
	</li>
	<li>
		<strong>FIZZ: Factual Inconsistency Detection by Zoom-in Summary and Zoom-out Document</strong>
		<a href="https://aclanthology.org/2024.emnlp-main.3.pdf">[pdf]</a>
		<br><i>Joonho Yang, <u>Seunghyun Yoon</u>, Byeongjeong Kim, Hwanhee Lee</i>
		<br><a href="https://2024.emnlp.org/">EMNLP 2024</a>
		<p>
	</li>
	<li>
		<strong>Towards Enhancing Coherence in Extractive Summarization: Dataset and Experiments with LLMs</strong>
		<a href="https://aclanthology.org/2024.emnlp-main.1106.pdf">[pdf]</a>
		<br><i>Mihir Parmar, Hanieh Deilamsalehy, Franck Dernoncourt, <u>Seunghyun Yoon</u>, Ryan A. Rossi, Trung Bui</i>
		<br><a href="https://2024.emnlp.org/">EMNLP 2024</a>
		<p>
	</li>
	<li>
		<strong>A New Framework for Evaluating Faithfulness of Video Moment Retrieval against Multiple Distractors</strong>
		<a href="https://dl.acm.org/doi/pdf/10.1145/3627673.3679838">[pdf]</a>
		<a href="https://github.com/yny0506/Massive-Videos-Moment-Retrieval">[code]</a>
		<br><i>Nakyeong Yang, Minsung Kim, <u>Seunghyun Yoon</u>, Joongbo Shin, Kyomin Jung</i>
		<br><a href="https://cikm2024.org/">CIKM 2024</a>
		<p>
	</li>
	<li>
		<strong>Identifying Speakers in Dialogue Transcripts: A Text-based Approach Using Pretrained Language Models</strong>
		<a href="https://www.isca-archive.org/interspeech_2024/nguyen24_interspeech.pdf">[pdf]</a>
		<br><i>Van Minh Nguyen, Franck Dernoncourt, <u>Seunghyun Yoon</u>, Hanieh Deilamsalehy, Hao Tan, Ryan Rossi, Quan Hung Tran, Trung Bui, Thien Nguyen</i>
		<br><a href="https://interspeech2024.org/">INTERSPEECH 2024</a>
		<p>
	</li>
	<li>
		<strong>Assessing News Thumbnail Representativeness: Counterfactual text can enhance the cross-modal matching ability</strong>
		<a href="https://aclanthology.org/2024.findings-acl.534.pdf">[pdf]</a>
		<a href="https://github.com/ssu-humane/news-images-acl24">[code]</a>
		<br><i>Yejun Yoon, <u>Seunghyun Yoon</u>, Kunwoo Park</i>
		<br><a href="https://2024.aclweb.org/">ACL 2024 Findings</a>
		<p>
	</li>
	<li>
		<strong>Multi-hop Database Reasoning with Virtual Knowledge Graph</strong>
		<a href="https://aclanthology.org/2024.kallm-1.1.pdf">[pdf]</a>
		<br><i>Juhee Son, Yeon Seonwoo, Alice Oh, James Thorne, <u>Seunghyun Yoon</u></i>
		<br><a href="https://2024.aclweb.org/">ACL 2024 Workshop KaLLM</a>
		<p>
	</li>
	<li>
		<strong>KaPQA: Knowledge-Augmented Product Question-Answering</strong>
		<a href="https://aclanthology.org/2024.knowledgenlp-1.2.pdf">[pdf]</a>
		<br><i>Swetha Eppalapally, Daksh Dangi, Chaithra Bhat, Ankita Gupta, Ruiyi Zhang, Karishma Bagga, <u>Seunghyun Yoon</u>, Nedim Lipka, Ryan A. Rossi, Franck Dernoncourt</i>
		<br><a href="https://2024.aclweb.org/">ACL 2024 Workshop KnowledgeNLP</a>
		<p>
	</li>
	<li>
		<strong>Multilingual Sentence-Level Semantic Search using Meta-Distillation Learning</strong>
		<a href="https://dl.acm.org/doi/pdf/10.1145/3626772.3657812">[pdf]</a>
		<br><i>Meryem M'hamdi, Jonathan May, Franck Dernoncourt, Trung Bui, <u>Seunghyun Yoon</u></i>
		<br><a href="https://sigir-2024.github.io/">SIGIR 2024</a>
		<p>
	</li>
	<li>
		<strong>PEEB: Part-based Bird Classifiers with an Explainable and Editable Language Bottleneck</strong>
		<a href="https://aclanthology.org/2024.findings-naacl.131.pdf">[pdf]</a>
		<br><i>Thang M. Pham, Peijie Chen, Tin Nguyen, <u>Seunghyun Yoon</u>, Trung Bui, Anh Nguyen</i>
		<br><a href="https://2024.naacl.org/">NAACL 2024 Findings</a>
		<p>
	</li>
	<li>
		<strong>Scaling Up Video Summarization Pretraining with Large Language Models</strong>
		<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Argaw_Scaling_Up_Video_Summarization_Pretraining_with_Large_Language_Models_CVPR_2024_paper.pdf">[pdf]</a>
		<br><i>Dawit Mureja Argaw, <u>Seunghyun Yoon</u>, Fabian Caba Heilbron, Hanieh Deilamsalehy, Trung Bui, Zhaowen Wang, Franck Dernoncourt, Joon Son Chung </i>
		<br><a href="https://cvpr.thecvf.com/">CVPR 2024</a>
		<p>
	</li>
	<li>
		<strong>Fine-tuning CLIP Text Encoders with Two-step Paraphrasing</strong>
		<a href="https://aclanthology.org/2024.findings-eacl.144.pdf">[pdf]</a>
		<br><i>Hyunjae Kim, <u>Seunghyun Yoon</u>, Trung Bui, Handong Zhao, Quan Tran, Franck Dernoncourt, Jaewoo Kang</i>
		<br><a href="https://2024.eacl.org/">EACL 2024 Findings</a>
		<p>
	</li>
	<li>
		<strong>Retrieval Augmented Generation for Domain-specific Question</strong>
		<a href="https://arxiv.org/pdf/2404.14760">[pdf]</a>
		<br><i>Sanat Sharma, <u>Seunghyun Yoon</u>, Franck Dernoncourt, Dewang Sultania, Karishma Bagga, Mengjiao Zhang, Trung Bui, Varun Kotte</i>
		<br><a href="https://sites.google.com/view/sdu-aaai24">AAAI 2024 Workshop SDU</a>
		<p>
	</li>
	<li>
		<strong>Multi-Modal Video Topic Segmentation with Dual-Contrastive Domain Adaptation</strong>
		<a href="https://arxiv.org/pdf/2312.00220.pdf">[pdf]</a>
		<br><i>Linzi Xing, Quan Tran, Fabian Caba Heilbron, Franck Dernoncourt, <u>Seunghyun Yoon</u>, Zhaowen Wang, Trung Bui, Giuseppe Carenini</i>
		<br><a href="https://mmm2024.org/">Multimedia Modelling 2024</a>
		<p>
	</li>
  <h4><strong>[2023]</strong></h4>
	<li>
		<strong>Aspect-based Meeting Transcript Summarization: A Two-Stage Approach with Weak Supervision on Sentence Classification</strong>
		<a href="https://arxiv.org/pdf/2311.04292.pdf">[pdf]</a>
		<br><i>Zhongfen Deng, <u>Seunghyun Yoon</u>, Trung Bui, Franck Dernoncourt, Quan Tran, Shuaiqi Liu, Wenting Zhao, Tao Zhang, Yibo Wang, Philip Yu</i>
		<br><a href="https://bigdataieee.org/BigData2023/">IEEE BigData 2023</a>
		<p>
	</li>
	<li>
		<strong>Perturbation Robust Metric for Multi-Lingual Image Captioning</strong>
		<a href="https://aclanthology.org/2023.findings-emnlp.819.pdf">[pdf]</a>
		<br><i>Yongil Kim, Yerin Hwang, Hyeongu Yun, <u>Seunghyun Yoon</u>, Trung Bui, Kyomin Jung</i>
		<br><a href="https://2023.emnlp.org/">EMNLP 2023 Findings</a>
		<p>
	</li>
	<li>
		<strong>Moment Detection in Long Tutorial Videos</strong>
		<a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Croitoru_Moment_Detection_in_Long_Tutorial_Videos_ICCV_2023_paper.pdf">[pdf]</a>
		<a href="https://github.com/ioanacroi/longmoment-detr">[code]</a>
		<br><i>Ioana Croitoru, Simion-Vlad Bogolin, Samuel Albanie, Yang Liu, Zhaowen Wang, <u>Seunghyun Yoon</u>, Franck Dernoncourt, Hailin Jin, Trung Bui</i>
		<br><a href="https://iccv2023.thecvf.com/">ICCV 2023</a>
		<p>
	</li>
	<li>
		<strong>Boosting Punctuation Restoration with Data Generation and Reinforcement Learning</strong>
		<a href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/lai23c_interspeech.pdf">[pdf]</a>
		<br><i>Viet Lai, Abel Salinas, Hao Tan, Trung Bui, Quan Tran, <u>Seunghyun Yoon</u>, Hanieh Deilamsalehy, Franck Dernoncourt, Thien Nguyen</i>
		<br><a href="https://www.interspeech2023.org/">INTERSPEECH 2023</a>
		<p>
	</li>
	<li>
		<strong>Automatic Creation of Named Entity Recognition Datasets by Querying Phrase Representations</strong>
		<a href="https://aclanthology.org/2023.acl-long.394.pdf">[pdf]</a>
		<br><i>Hhynjae Kim, Jaehyo Yoo, <u>Seunghyun Yoon</u>, Jaewoo Kang</i>
		<br><a href="https://2023.aclweb.org/">ACL 2023</a>
		<p>
	</li>
	<li>
		<strong>MEETINGQA: Extractive Question-Answering on Meeting Transcripts</strong>
		<a href="https://aclanthology.org/2023.acl-long.837.pdf">[pdf]</a>
		<br><i>Archiki Prasad, Trung Bui, <u>Seunghyun Yoon</u>, Hanieh Deilamsalehy, Franck Dernoncourt, Mohit Bansal</i>
		<br><a href="https://2023.aclweb.org/">ACL 2023</a>
		<p>
	</li>
	<li>
		<strong>PiC: A Phrase-in-Context Dataset for Phrase Understanding and Semantic Search</strong>
		<a href="https://aclanthology.org/2023.eacl-main.1.pdf">[pdf]</a>
		<a href="https://phrase-in-context.github.io/">[page]</a>
		<br><i>Thang M. Pham, <u>Seunghyun Yoon</u>, Trung Bu, Anh Nguyeng</i>
		<br><a href="https://2023.eacl.org/">EACL 2023</a>
		<p>
	</li>
  <h4><strong>[2022]</strong></h4>
	<li>
		<strong>Factual Error Correction for Abstractive Summaries Using Entity Retrieval</strong>
		<a href="https://aclanthology.org/2022.gem-1.41.pdf">[pdf]</a>
		<br><i>Hwanhee Lee, Cheoneum Park, <u>Seunghyun Yoon</u>, Trung Bu, Franck Dernoncourt, Juae Kim, Kyomin Jung</i>
		<br><a href="https://2022.emnlp.org/">EMNLP 2022 Workshop GEM</a>
		<p>
	</li>
	<li>
		<strong>Improving cross-modal attention via object detection</strong>
		<a href="https://attention-learning-workshop.github.io/2022/papers/kim-improving_crossmodal_attention_via_object_detection.pdf">[pdf]</a>
		<br><i>Yongil Kim, Yerin Hwang, <u>Seunghyun Yoon</u>, Hyeongu Yun, Kyomin Jung</i>
		<br><a href="https://nips.cc/">NeurIPS 2022 Workshop All Things Attention</a>
		<p>
	</li>
	<li>
		<strong>Simple Questions Generate Named Entity Recognition Datasets</strong>
		<a href="https://aclanthology.org/2022.emnlp-main.417.pdf">[pdf]</a>
		<a href="https://github.com/dmis-lab/GeNER">[code]</a>
		<br><i>Hyunjae Kim, Jaehyo Yoo, <u>Seunghyun Yoon</u>, Jinhyuk Lee, Jaewoo Kang</i>
		<br><a href="https://2022.emnlp.org/">EMNLP 2022</a>
		<p>
	</li>
	<li>
		<strong>Virtual Knowledge Graph Construction for Zero-Shot Domain-Specific Document Retrieval</strong>
		<a href="https://aclanthology.org/2022.coling-1.101.pdf">[pdf]</a>
		<br><i>Yeon Seonwoo, <u>Seunghyun Yoon</u>, Franck Dernoncourt, Trung Bui, Alice Oh</i>
		<br><a href="https://coling2022.org/">COLING 2022</a>
		<p>
	</li>
	<li>
		<strong>Medical Question Understanding and Answering with Knowledge Grounding and Semantic Self-Supervision</strong>
		<a href="https://aclanthology.org/2022.coling-1.241.pdf">[pdf]</a>
		<br><i>Khalil Mrini, Harpreet Singh, Franck Dernoncourt, <u>Seunghyun Yoon</u>, Trung Bui, Walter W. Chang, Emilia Farcas, Ndapa Nakashole</i>
		<br><a href="https://coling2022.org/">COLING 2022</a>
		<p>
	</li>
	<li>
		<strong>Offensive Content Detection Via Synthetic Code-Switched Text</strong>
		<a href="https://aclanthology.org/2022.coling-1.575.pdf">[pdf]</a>
		<br><i>Cesa Salaam, Franck Dernoncourt, Trung Bui, <u>Seunghyun Yoon</u></i>
		<br><a href="https://coling2022.org/">COLING 2022</a>
		<p>
	</li>
	<li>
		<strong>Keyphrase Prediction from Video Transcripts: New Dataset and Directions</strong>
		<a href="https://aclanthology.org/2022.coling-1.624.pdf">[pdf]</a>
		<br><i>Amir Pouran Ben Veyseh, Quan Tran, <u>Seunghyun Yoon</u>, Varun Manjunatha, Hanieh Deilamsalehy, Rajiv Jain, Trung Bui, Walter W. Chang, Franck Dernoncourt, Thien Huu Nguyen</i>
		<br><a href="https://coling2022.org/">COLING 2022</a>
		<p>
	</li>
	<li>
		<strong>MACRONYM: A Large-Scale Dataset for Multilingual and Multi-Domain Acronym Extraction </strong>
		<a href="https://aclanthology.org/2022.coling-1.292.pdf">[pdf]</a>
		<br><i>Amir Pouran Ben Veyseh, Nicole Meister, <u>Seunghyun Yoon</u>, Rajiv Jain, Franck Dernoncourt, Thien Huu Nguyen</i>
		<br><a href="https://coling2022.org/">COLING 2022</a>
		<p>
	</li>
	<li>
		<strong>Fine-grained Image Captioning with CLIP Reward</strong>
		<a href="https://aclanthology.org/2022.findings-naacl.39.pdf">[pdf]</a>
		<a href="https://github.com/j-min/CLIP-Caption-Reward">[code]</a>
		<a href="https://huggingface.co/spaces/NAACL2022/CLIP-Caption-Reward">[demo]</a>
		<br><i>Jaemin Cho, <u>Seunghyun Yoon</u>, Ajinkya Kale, Franck Dernoncourt, Trung Bui, M Bansal</i>
		<br><a href="https://2022.naacl.org/">NAACL 2022 Findings</a>
		<p>
	</li>
	<li>
		<strong>Multimodal Intent Discovery from Livestream Videos</strong>
		<a href="https://aclanthology.org/2022.findings-naacl.36.pdf">[pdf]</a>
		<a href="https://github.com/adymaharana/VideoIntentDiscovery">[code]</a>
		<br><i>Adyasha Maharana, Quan Tran, <u>Seunghyun Yoon</u>, Franck Dernoncourt, Trung Bui, Walter Chang, M Bansal</i>
		<br><a href="https://2022.naacl.org/">NAACL Findings 2022</a>
		<p>
	</li>
	<li>
		<strong>How does fake news use a thumbnail? CLIP-based Multimodal Detection on the Unrepresentative News Image</strong>
		<a href="https://aclanthology.org/2022.constraint-1.10.pdf">[pdf]</a>
		<a href="https://github.com/ssu-humane/fake-news-thumbnail?tab=readme-ov-file">[code]</a>
		<br><i>Hyewon Choi, Yejun Yoon, <u>Seunghyun Yoon</u>, Kunwoo Park</i>
		<br><a href="https://lcs2.iiitd.edu.in/CONSTRAINT-2022/">ACL CONSTRAINT 2022</a>
		<p>
	</li>
	<li>
		<strong>CAISE: Conversational Agent for Image Search and Editing</strong>
		<a href="https://cdn.aaai.org/ojs/21337/21337-13-25350-1-2-20220628.pdf">[pdf]</a>
		<a href="https://github.com/hyounghk/CAISE">[code]</a>
		<br><i>Hyounghun Kim, Doo Soon Kim, <u>Seunghyun Yoon</u>, Franck Dernoncourt, Trung Bui, Mohit Bansal</i>
		<br><a href="https://aaai.org/Conferences/AAAI-22/">AAAI 2022</a>
		<p>
	</li>
  <h4><strong>[2021]</strong></h4>
	<li>
		<strong>Few-Shot Intent Detection via Contrastive Pre-Training and Fine-Tuning</strong>
		<a href="https://aclanthology.org/2021.emnlp-main.144.pdf">[pdf]</a>
		<br><i>J Zhang, T Bui, <u>S Yoon</u>, X Chen, Z Liu, C Xia, QH Tran, W Chang, P Yue</i>
		<br><a href="https://2021.emnlp.org/">EMNLP 2021</a>
		<p>
	</li>
	<li>
		<strong>QACE: Asking Questions to Evaluate an Image Caption</strong>
		<a href="https://aclanthology.org/2021.findings-emnlp.395.pdf">[pdf]</a>
		<a href="https://github.com/hwanheelee1993/QACE">[code]</a>
		<br><i>H Lee, T Scialom, <u>S Yoon</u>, F Dernoncourt, K Jung</i>
		<br><a href="https://2021.emnlp.org/">EMNLP 2021 Findings</a>
		<p>
	</li>
	<li>
		<strong>A Gradually Soft Multi-Task and Data-Augmented Approach to Medical Question Understanding</strong>
		<a href="https://aclanthology.org/2021.acl-long.119.pdf">[pdf]</a>
		<br><i>K Mrini, F Dernoncourt, <u>S Yoon</u>, T Bui, W Chang, E Farcas, N Nakashole</i>
		<br><a href="https://2021.aclweb.org/">ACL 2021</a>
		<p>
	</li>
	<li>
		<strong>UMIC: An Unreferenced Metric for Image Captioning via Contrastive Learning</strong>
		<a href="https://aclanthology.org/2021.acl-short.29.pdf">[pdf]</a>
		<a href="https://github.com/hwanheelee1993/UMIC">[code]</a>
		<br><i>H Lee, <u>S Yoon</u>, F Dernoncourt, T Bui, K Jung</i>
		<br><a href="https://2021.aclweb.org/">ACL 2021</a>
		<p>
	</li>
	<li>
		<strong>UCSD-Adobe at MEDIQA 2021: Transfer Learning and Answer Sentence Selection for Medical Summarization</strong>
		<a href="https://www.aclweb.org/anthology/2021.bionlp-1.28.pdf">[pdf]</a>
		<br><i>K Mrini, F Dernoncourt, <u>S Yoon</u>, T Bui, W Chang, E Farcas, N Nakashole</i>
		<br><a href="https://aclweb.org/aclwiki/BioNLP_Workshop">NAACL 2021 Workshop BioNLP</a>
		<p>
	</li>
	<li>
		<strong>KPQA: A Metric for Generative Question Answering Using Keyphrase Weights</strong>
		<a href="https://www.aclweb.org/anthology/2021.naacl-main.170.pdf">[pdf]</a>
		<a href="https://github.com/hwanheelee1993/KPQA">[code]</a>
		<br><i>H Lee, <u>S Yoon</u>, F Dernoncourt, DS Kim, T Bui, J Shin, K Jung</i>
		<br><a href="https://2021.naacl.org/">NAACL 2021</a>
		<p>
	</li>
	<li>
		<strong>Learning to Detect Incongruence in News Headline and Body Text via a Graph Neural Network</strong>
		<a href="https://ieeexplore.ieee.org/document/9363185">[pdf]</a>
		<a href="https://github.com/minwhoo/detecting-incongruity-gnn">[code]</a>
		<br><font color=orange>(SCI, IF=3.745)</font>
		<br><i><u>S Yoon</u>*, K Park*, M Lee, T Kim, M Cha, K Jung</i>
		<br><a href="https://ieeeaccess.ieee.org/">IEEE Access 2021</a>
		<p>
	</li>
  <h4><strong>[2020]</strong></h4>
	<li>
		<strong>Collaborative Training of GANs in Continuous and Discrete Spaces for Text Generation</strong>
		<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9296209">[pdf]</a>
		<br><font color=orange>(SCI, IF=3.745)</font>
		<br><i>Y Kim, S Won, <u>S Yoon</u>, K Jung</i>
		<br><a href="https://ieeeaccess.ieee.org/">IEEE Access 2020</a>
		<p>
	</li>
	<li>
		<strong>ViLBERTScore: Evaluating Image Caption Using Vision-and-Language BERT</strong>
		<a href="https://www.aclweb.org/anthology/2020.eval4nlp-1.4.pdf">[pdf]</a>
		<a href="https://github.com/hwanheelee1993/ViLBERTScore">[code]</a>
		<br><i>H Lee, <u>S Yoon</u>, F Dernoncourt, DS Kim, T Bui, K Jung</i>
		<br><a href="https://nlpevaluation2020.github.io/index.html">EMNLP 2020 Workshop Eval4NLP</a>
		<p>
	</li>
	<li>
		<strong>Multimodal Speech Emotion Recognition using Cross Attention with Aligned Audio and Text</strong>
		<a href="https://www.isca-archive.org/interspeech_2020/lee20e_interspeech.pdf">[pdf]</a>
		<br><i>Y Lee, <u>S Yoon</u>, K Jung</i>
		<br><a href="https://arxiv.org/abs/2207.12895/">INTERSPEECH 2020</a>
		<p>
	</li>
	<li>
		<strong>Fast and Accurate Deep Bidirectional Language Representations for Unsupervised Learning</strong>
		<a href="https://www.aclweb.org/anthology/2020.acl-main.76.pdf">[pdf]</a>
		<br><i>J Shin, Y Lee, <u>S Yoon</u>, K Jung</i>
		<br><a href="https://acl2020.org/">ACL 2020</a>
		<p>
	</li>
	<li>
		<strong>Propagate-Selector: Detecting Supporting Sentences for Question Answering via Graph Neural Networks</strong>
		<a href="https://www.aclweb.org/anthology/2020.lrec-1.664">[pdf]</a>
		<a href="https://github.com/david-yoon/propagate-selector">[code]</a>
		<br><i><u>S Yoon</u>, F Dernoncourt, DS Kim, T Bui, K Jung</i>
		<br><a href="https://lrec2020.lrec-conf.org/">LREC 2020</a>
		<p>
	</li>
	<li>
		<strong>Drug-disease Graph: Predicting Adverse Drug Reaction Signals via Graph Neural Network with Clinical Data</strong>
		<a href="https://arxiv.org/pdf/2004.00407.pdf">[pdf]</a>
		<a href="https://pakdd2020.org/download/conference_paper_slides/main-851.pdf">[slide]</a>
		<br><font color=orange>(oral presentation)</font>
		<br><i>H Kwak, M Lee, <u>S Yoon</u>, J Chang, S Park, K Jung</i>
		<br><a href="https://pakdd2020.org/">PAKDD 2020</a>
		<p>
	</li>
	<li>
		<strong>DSTC8-AVSD: Multimodal Semantic Transformer Network with Retrieval Style Word Generator</strong>
		<a href="https://arxiv.org/pdf/2004.08299.pdf">[pdf]</a>
		<br><i>H Lee, <u>S Yoon</u>, F Dernoncourt, DS Kim, T Bui, K Jung</i>
		<br><a href="https://aaai.org/Conferences/AAAI-20/ws20workshops/#ws09">AAAI 2020  Workshop DSTC8</a>
		<p>
	</li>
	<li>
		<strong>Comparative Studies on Machine Learning for Paralinguistic Signal Compression and Classification</strong>
		<a href="https://link.springer.com/content/pdf/10.1007/s11227-020-03346-3.pdf">[pdf]</a>
		<br><font color=orange>(SCI, IF=2.157)</font>
		<br><i>S Byun*, <u>S Yoon</u>*, K Jung</i>
		<br><a href="https://www.springer.com/journal/11227/?gclid=EAIaIQobChMIrJqOzti96QIVBhdgCh1UVAn_EAAYASAAEgLwp_D_BwE/">Journal of Supercomputing 2020</a>
		<p>
	</li>
	<li>
		<strong>Attentive Modality Hopping Mechanism for Speech Emotion Recognition</strong>
		<a href="https://david-yoon.github.io/assets/paper/yoon2020attentive.pdf">[pdf]</a>
		<a href="https://github.com/david-yoon/attentive-modality-hopping-for-SER">[code]</a>
		<a href="https://www.slideshare.net/DavidSeunghyunYoon/slide-attentive-modality-hopping-mechanism-for-speech-emotion-recognition">[slide]</a>
		<br><font color=orange>(oral presentation)</font>
		<br><i><u>S Yoon</u>, S Dey, H Lee, K Jung</i>
		<br><a href="https://2020.ieeeicassp.org/">IEEE ICASSP 2020</a>
		<p>
	</li>
	<li>
		<strong>BaitWatcher: A lightweight web interface for the detection of incongruent news headlines</strong>
		<a href="https://arxiv.org/pdf/2003.11459.pdf">[pdf]</a>
		<a href="https://www.springer.com/gp/book/9783030426989">[book]</a>
		<br><i>K Park, T Kim, <u>S Yoon</u>, M Cha, K Jung</i>
		<br><a href="https://www.springer.com/gp/book/9783030426989">Disinformation, Misinformation, and Fake News in Social Media-Emerging Research Challenges and Opportunities, Springer 2020</a>
		<p>
	</li>
  <h4><strong>[2019]</strong></h4>
	<li>
		<strong>A Compare-Aggregate Model with Latent Clustering for Answer Selection</strong>
		<a href="https://arxiv.org/pdf/1905.12897.pdf">[pdf]</a>
		<a href="https://www.slideshare.net/DavidSeunghyunYoon/slide-a-compareaggregate-model-with-latent-clustering-for-answer-selection">[slide]</a>
		<a href="https://www.slideshare.net/DavidSeunghyunYoon/poster-a-compareaggregate-model-with-latent-clustering-for-answer-selection">[poster]</a>
		<br><font color=orange>(oral presentation)</font>
		<br><i><u>S Yoon</u>, F Dernoncourt, DS Kim, T Bui, K Jung</i>
		<br><a href="http://www.cikm2019.net/">CIKM 2019</a>
		<p>
	</li>
	<li>
		<strong>Surf at MEDIQA 2019: Improving Performance of Natural Language Inference in the Clinical Domain by Adopting Pre-trained Language Model</strong>
		<a href="https://www.aclweb.org/anthology/W19-5043.pdf">[pdf]</a>
		<br><i>J Nam, <u>S Yoon</u>, K Jung</i>
		<br><a href="https://aclweb.org/aclwiki/BioNLP_Workshop">ACL 2019 Workshop BioNLP</a>
		<p>
	</li>
	<li>
		<strong>Speech Emotion Recognition Using Multi-hop Attention Mechanism</strong>
		<a href="https://arxiv.org/pdf/1904.10788.pdf">[pdf]</a>
		<a href="https://sigport.org/documents/speech-emotion-recognition-using-multi-hop-attention-mechanism">[slide]</a>
		<br><font color=orange>(oral presentation)</font>
		<br><i><u>S Yoon</u>, S Byun, S Dey, K Jung</i>
		<br><a href="https://2019.ieeeicassp.org/">IEEE ICASSP 2019</a>
		<p>
	</li>
	<li>
		<strong>Neural Networks for Compressing and Classifying Speaker-Independent Paralinguistic Signals</strong>
		<a href="http://milab.snu.ac.kr/pub/BigComp2019Byun.pdf">[pdf]</a>
		<br><i>S Byun, <u>S Yoon</u>, K Jung</i>
		<br><a href="http://www.bigcomputing.org/">IEEE BigComp 2019</a>
		<p>
	</li>
	<li>
		<strong>Detecting Incongruity Between News Headline and Body Text via a Deep Hierarchical Encoder </strong>
		<a href="https://wvvw.aaai.org/ojs/index.php/AAAI/article/view/3756/3634">[pdf]</a>
		<a href="https://github.com/david-yoon/detecting-incongruity">[code]</a>
		<br><font color=orange>(oral presentation)</font>
		<br><i><u>S Yoon</u>*, K Park*, J Shin, H Lim, S Won, M Cha, K Jung</i>
		<br><a href="https://aaai.org/Conferences/AAAI-19/">AAAI 2019</a>
		<p>
	</li>
  <h4><strong>[2018 and earlier]</strong></h4>
	<li>
		<strong>Multimodal Speech Emotion Recognition using Audio and Text </strong>
		<a href="https://arxiv.org/pdf/1810.04635.pdf">[pdf]</a>
		<a href="https://github.com/david-yoon/multimodal-speech-emotion">[code]</a>
		<br><i><u>S Yoon</u>, S Byun, K Jung</i>
		<br><a href="http://www.slt2018.org/">IEEE SLT 2018</a>
		<p>
	</li>
	<li>
		<strong>Comparative Studies of Detecting Abusive Language on Twitter</strong>
		<a href="https://www.aclweb.org/anthology/W18-5113.pdf">[pdf]</a>
		<a href="https://github.com/younggns/comparative-abusive-lang">[code]</a>
		<br><i>Y Lee*, <u>S Yoon</u>*, K Jung</i>
		<br><a href="https://sites.google.com/view/alw2018">EMNLP ALW 2018</a>
		<p>
	</li>
	<li>
		<strong>Learning to Rank Question-Answer Pairs using Hierarchical Recurrent Encoder with Latent Topic Clustering</strong>
		<a href="https://www.aclweb.org/anthology/N18-1142.pdf">[pdf]</a>
		<a href="https://github.com/david-yoon/QA_HRDE_LTC">[code]</a>
		<br><i><u>S Yoon</u>, J Shin, K Jung</i>
		<br><a href="http://naacl2018.org/">NAACL 2018</a>
		<p>
	</li>
	<li>
		<strong>Contextual-CNN: A Novel Architecture Capturing Unified Meaning for Sentence Classification</strong>
		<a href="http://milab.snu.ac.kr/pub/BigComp2018.pdf">[pdf]</a>
		<br><i>J Shin, Y Kim, <u>S Yoon</u>, K Jung</i>
		<br><a href="http://www.bigcomputing.org/">IEEE BigComp 2018</a>
		<p>
	</li>
	<li>
		<strong>Synonym Discovery with Etymology-based Word Embeddings</strong>
		<a href="https://arxiv.org/pdf/1709.10445.pdf">[pdf]</a>
		<br><i><u>S Yoon</u>, P Estrada, K Jung</i>
		<br><a href="http://www.ele.uri.edu/ieee-ssci2017/">IEEE SSCI 2017</a>
		<p>
	</li>
	<li>
		<strong>Efficient Transfer Learning Schemes for Personalized Language Modeling using Recurrent Neural Network</strong>
		<a href="http://milab.snu.ac.kr/pub/AAAI2017Yoon.pdf">[pdf]</a>
		<br><i><u>S Yoon</u>, H Yun, Y Kim, G Park, K Jung</i>
		<br><a href="http://crowdai.azurewebsites.net/">AAAI 2017 (Workshop)</a>
		<p>
	</li>
	<li>
		<strong>Automatic Question Answering System for Consumer Product</strong>
		<a href="http://milab.snu.ac.kr/pub/IntelliSys2016Yoon.pdf">[pdf]</a>
		<br><i><u>S Yoon</u>, M Sundar, A Gupta, K Jung</i>
		<br><a href="http://saiconference.com/Conferences/IntelliSys2016%22">IntelliSys 2016</a>
		<p>
	</li>
	<li>
		<strong>Mining the Minds of Customers from Online Chat Logs</strong>
		<a href="https://arxiv.org/abs/1510.01801">[pdf]</a>
		<br><i>K Park, J Kim, J Park, M Cha, J Nam, <u>S Yoon</u>, E Rhim</i>
		<br><a href="http://www.cikm-2015.org/">CIKM 2015</a>
		<p>
	</li>
	<li>
		<strong>Domain Question Answering System</strong>
		<a href="nan">[pdf]</a>
		<br><i><u>S Yoon</u>, E Rhim, D Kim</i>
		<br><a href="nan">KIISE Transactions on Computing Practices 2015</a>
		<p>
	</li>
	<li>
		<strong>Media clips: Implementation of an intuitive media linker</strong>
		<a href="https://david-yoon.github.io/assets/paper/MediaClip_BMSB2011.pdf">[pdf]</a>
		<br><i><u>S Yoon</u>, K Lee, H Shin</i>
		<br><a href="https://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp?punumber=5945017">IEEE BMSB 2011</a>
		<p>
	</li>
  </font>
</ol>
